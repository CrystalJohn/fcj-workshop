<!doctype html><html lang=en class="js csstransforms3d"><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=generator content="Hugo 0.134.3"><meta name=description content><meta name=author content="phuctptse183992@fpt.edu.vn"><link rel=icon href=../../images/favicon.png type=image/png><title>Build end-to-end Apache Spark pipelines with Amazon MWAA, Batch Processing Gateway, and Amazon EMR on EKS :: Internship Report</title>
<link href=../../css/nucleus.css?1765512673 rel=stylesheet><link href=../../css/fontawesome-all.min.css?1765512673 rel=stylesheet><link href=../../css/hybrid.css?1765512673 rel=stylesheet><link href=../../css/featherlight.min.css?1765512673 rel=stylesheet><link href=../../css/perfect-scrollbar.min.css?1765512673 rel=stylesheet><link href=../../css/auto-complete.css?1765512673 rel=stylesheet><link href=../../css/atom-one-dark-reasonable.css?1765512673 rel=stylesheet><link href=../../css/theme.css?1765512673 rel=stylesheet><link href=../../css/hugo-theme.css?1765512673 rel=stylesheet><link href=../../css/theme-workshop.css?1765512673 rel=stylesheet><script src=../../js/jquery-3.3.1.min.js?1765512673></script><style>:root #header+#content>#left>#rlblock_left{display:none!important}</style></head><body data-url=../../3-blogstranslated/3.2-blog2/><nav id=sidebar class=showVisitedLinks><div id=header-wrapper><div id=header><a id=logo href=../../><svg id="Layer_1" data-name="Layer 1" viewBox="0 0 60 30" width="30%"><defs><style>.cls-1{fill:#fff}.cls-2{fill:#f90;fill-rule:evenodd}</style></defs><title>AWS-Logo_White-Color</title><path class="cls-1" d="M14.09 10.85a4.7 4.7.0 00.19 1.48 7.73 7.73.0 00.54 1.19.77.77.0 01.12.38.64.64.0 01-.32.49l-1 .7a.83.83.0 01-.44.15.69.69.0 01-.49-.23 3.8 3.8.0 01-.6-.77q-.25-.42-.51-1a6.14 6.14.0 01-4.89 2.3 4.54 4.54.0 01-3.32-1.19 4.27 4.27.0 01-1.22-3.2 4.28 4.28.0 011.46-3.4A6.06 6.06.0 017.69 6.46a12.47 12.47.0 011.76.13q.92.13 1.91.36V5.73a3.65 3.65.0 00-.79-2.66A3.81 3.81.0 007.86 2.3a7.71 7.71.0 00-1.79.22 12.78 12.78.0 00-1.79.57 4.55 4.55.0 01-.58.22h-.26q-.35.0-.35-.52V2a1.09 1.09.0 01.12-.58 1.2 1.2.0 01.47-.35A10.88 10.88.0 015.77.32 10.19 10.19.0 018.36.0a6 6 0 014.35 1.35 5.49 5.49.0 011.38 4.09zM7.34 13.38a5.36 5.36.0 001.72-.31A3.63 3.63.0 0010.63 12 2.62 2.62.0 0011.19 11a5.63 5.63.0 00.16-1.44v-.7a14.35 14.35.0 00-1.53-.28 12.37 12.37.0 00-1.56-.1 3.84 3.84.0 00-2.47.67A2.34 2.34.0 005 11a2.35 2.35.0 00.61 1.76A2.4 2.4.0 007.34 13.38zm13.35 1.8a1 1 0 01-.64-.16 1.3 1.3.0 01-.35-.65L15.81 1.51a3 3 0 01-.15-.67.36.36.0 01.41-.41H17.7a1 1 0 01.65.16 1.4 1.4.0 01.33.65l2.79 11 2.59-11A1.17 1.17.0 0124.39.6a1.1 1.1.0 01.67-.16H26.4a1.1 1.1.0 01.67.16 1.17 1.17.0 01.32.65L30 12.39 32.88 1.25A1.39 1.39.0 0133.22.6a1 1 0 01.65-.16h1.54a.36.36.0 01.41.41 1.36 1.36.0 010 .26 3.64 3.64.0 01-.12.41l-4 12.86a1.3 1.3.0 01-.35.65 1 1 0 01-.64.16H29.25a1 1 0 01-.67-.17 1.26 1.26.0 01-.32-.67L25.67 3.64l-2.56 10.7a1.26 1.26.0 01-.32.67 1 1 0 01-.67.17zm21.36.44a11.28 11.28.0 01-2.56-.29 7.44 7.44.0 01-1.92-.67 1 1 0 01-.61-.93v-.84q0-.52.38-.52a.9.9.0 01.31.06l.42.17a8.77 8.77.0 001.83.58 9.78 9.78.0 002 .2 4.48 4.48.0 002.43-.55 1.76 1.76.0 00.86-1.57 1.61 1.61.0 00-.45-1.16A4.29 4.29.0 0043 9.22l-2.41-.76A5.15 5.15.0 0138 6.78a3.94 3.94.0 01-.83-2.41 3.7 3.7.0 01.45-1.85 4.47 4.47.0 011.19-1.37 5.27 5.27.0 011.7-.86A7.4 7.4.0 0142.6.0a8.87 8.87.0 011.12.07q.57.07 1.08.19t.95.26a4.27 4.27.0 01.7.29 1.59 1.59.0 01.49.41.94.94.0 01.15.55v.79q0 .52-.38.52a1.76 1.76.0 01-.64-.2 7.74 7.74.0 00-3.2-.64 4.37 4.37.0 00-2.21.47 1.6 1.6.0 00-.79 1.48 1.58 1.58.0 00.49 1.18 4.94 4.94.0 001.83.92L44.55 7a5.08 5.08.0 012.57 1.6A3.76 3.76.0 0147.9 11a4.21 4.21.0 01-.44 1.93 4.4 4.4.0 01-1.21 1.47 5.43 5.43.0 01-1.85.93A8.25 8.25.0 0142.05 15.62z"/><path class="cls-2" d="M45.19 23.81C39.72 27.85 31.78 30 25 30A36.64 36.64.0 01.22 20.57c-.51-.46-.06-1.09.56-.74A49.78 49.78.0 0025.53 26.4 49.23 49.23.0 0044.4 22.53C45.32 22.14 46.1 23.14 45.19 23.81z"/><path class="cls-2" d="M47.47 21.21c-.7-.9-4.63-.42-6.39-.21-.53.06-.62-.4-.14-.74 3.13-2.2 8.27-1.57 8.86-.83s-.16 5.89-3.09 8.35c-.45.38-.88.18-.68-.32C46.69 25.8 48.17 22.11 47.47 21.21z"/></svg></a></div><div class=searchbox><label for=search-by><i class="fas fa-search"></i></label>
<input data-search-input id=search-by type=search placeholder=Search...>
<span data-search-clear><i class="fas fa-times"></i></span></div><script type=text/javascript src=../../js/lunr.min.js?1765512673></script><script type=text/javascript src=../../js/auto-complete.js?1765512673></script><script type=text/javascript>var baseurl="https://crystaljohn.github.io/fcj-workshop/"</script><script type=text/javascript src=../../js/search.js?1765512673></script></div><div class=highlightable><ul class=topics><li data-nav-id=/1-worklog/ title=Worklog class=dd-item><a href=../../1-worklog/><b>1. </b>Worklog
<i class="fas fa-check read-icon"></i></a><ul><li data-nav-id=/1-worklog/1.7-week7/ title="Week 7 Worklog" class=dd-item><a href=../../1-worklog/1.7-week7/><b>1.7. </b>Week 7 Worklog
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/1-worklog/1.1-week1/ title="Week 1 Worklog" class=dd-item><a href=../../1-worklog/1.1-week1/><b>1.1. </b>Week 1 Worklog
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/1-worklog/1.2-week2/ title="Week 2 Worklog" class=dd-item><a href=../../1-worklog/1.2-week2/><b>1.2. </b>Week 2 Worklog
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/1-worklog/1.3-week3/ title="Week 3 Worklog" class=dd-item><a href=../../1-worklog/1.3-week3/><b>1.3. </b>Week 3 Worklog
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/1-worklog/1.4-week4/ title="Week 4 Worklog" class=dd-item><a href=../../1-worklog/1.4-week4/><b>1.4. </b>Week 4 Worklog
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/1-worklog/1.5-week5/ title="Week 5 Worklog" class=dd-item><a href=../../1-worklog/1.5-week5/><b>1.5. </b>Week 5 Worklog
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/1-worklog/1.6-week6/ title="Week 6 Worklog" class=dd-item><a href=../../1-worklog/1.6-week6/><b>1.6. </b>Week 6 Worklog
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/1-worklog/1.8-week8/ title="Week 8 Worklog" class=dd-item><a href=../../1-worklog/1.8-week8/><b>1.8. </b>Week 8 Worklog
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/1-worklog/1.9-week9/ title="Week 9 Worklog" class=dd-item><a href=../../1-worklog/1.9-week9/><b>1.9. </b>Week 9 Worklog
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/1-worklog/1.10-week10/ title="Week 10 Worklog" class=dd-item><a href=../../1-worklog/1.10-week10/><b>1.10. </b>Week 10 Worklog
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/1-worklog/1.11-week11/ title="Week 11 Worklog" class=dd-item><a href=../../1-worklog/1.11-week11/><b>1.11. </b>Week 11 Worklog
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/1-worklog/1.12-week12/ title="Week 12 Worklog" class=dd-item><a href=../../1-worklog/1.12-week12/><b>1.12 </b>Week 12 Worklog
<i class="fas fa-check read-icon"></i></a></li></ul></li><li data-nav-id=/2-proposal/ title=Proposal class=dd-item><a href=../../2-proposal/><b>2. </b>Proposal
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/3-blogstranslated/ title="Translated Blogs" class="dd-item
parent"><a href=../../3-blogstranslated/><b>3. </b>Translated Blogs
<i class="fas fa-check read-icon"></i></a><ul><li data-nav-id=/3-blogstranslated/3.1-blog1/ title="Build a Virtual Meteorologist Using Amazon Bedrock Agents" class=dd-item><a href=../../3-blogstranslated/3.1-blog1/><b>3.1. </b>Build a Virtual Meteorologist Using Amazon Bedrock Agents
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/3-blogstranslated/3.2-blog2/ title="Build end-to-end Apache Spark pipelines with Amazon MWAA, Batch Processing Gateway, and Amazon EMR on EKS" class="dd-item
active"><a href=../../3-blogstranslated/3.2-blog2/><b>3.2. </b>Build end-to-end Apache Spark pipelines with Amazon MWAA, Batch Processing Gateway, and Amazon EMR on EKS
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/3-blogstranslated/3.3-blog3/ title="Building Golden Images with CIS Linux Build Kit in Amazon EC2 Image Builder" class=dd-item><a href=../../3-blogstranslated/3.3-blog3/><b>3.3. </b>Building Golden Images with CIS Linux Build Kit in Amazon EC2 Image Builder
<i class="fas fa-check read-icon"></i></a></li></ul></li><li data-nav-id=/4-eventparticipated/ title="Events Participated" class=dd-item><a href=../../4-eventparticipated/><b>4. </b>Events Participated
<i class="fas fa-check read-icon"></i></a><ul><li data-nav-id=/4-eventparticipated/4.1-event1/ title="Event 1" class=dd-item><a href=../../4-eventparticipated/4.1-event1/><b>4.1. </b>Event 1
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/4-eventparticipated/4.2-event2/ title="Event 2" class=dd-item><a href=../../4-eventparticipated/4.2-event2/><b>4.2. </b>Event 2
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/4-eventparticipated/4.3-event3/ title="Event 3" class=dd-item><a href=../../4-eventparticipated/4.3-event3/><b>4.2. </b>Event 3
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/4-eventparticipated/4.4-event4/ title="Event 4" class=dd-item><a href=../../4-eventparticipated/4.4-event4/><b>4.2. </b>Event 4
<i class="fas fa-check read-icon"></i></a></li></ul></li><li data-nav-id=/5-workshop/ title=Workshop class=dd-item><a href=../../5-workshop/><b>5. </b>Workshop
<i class="fas fa-check read-icon"></i></a><ul><li data-nav-id=/5-workshop/1-introduction/ title=Introduction class=dd-item><a href=../../5-workshop/1-introduction/><b>1. </b>Introduction
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/5-workshop/2-preparation/ title=Preparation class=dd-item><a href=../../5-workshop/2-preparation/><b>2. </b>Preparation
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/5-workshop/3-bedrock-models/ title="Activate Bedrock Models" class=dd-item><a href=../../5-workshop/3-bedrock-models/><b>3. </b>Activate Bedrock Models
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/5-workshop/4-aws-cli/ title="Configure AWS CLI" class=dd-item><a href=../../5-workshop/4-aws-cli/><b>4. </b>Configure AWS CLI
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/5-workshop/5-data-preparation/ title="Data Preparation" class=dd-item><a href=../../5-workshop/5-data-preparation/><b>5. </b>Data Preparation
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/5-workshop/6-infrastructure/ title="Deploy Infrastructure" class=dd-item><a href=../../5-workshop/6-infrastructure/><b>6. </b>Deploy Infrastructure
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/5-workshop/7-backend/ title="Set up Backend API" class=dd-item><a href=../../5-workshop/7-backend/><b>7. </b>Set up Backend API
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/5-workshop/8-idp-pipeline/ title="Set up IDP Pipeline" class=dd-item><a href=../../5-workshop/8-idp-pipeline/><b>8. </b>Set up IDP Pipeline
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/5-workshop/9-frontend/ title="Setup Frontend" class=dd-item><a href=../../5-workshop/9-frontend/><b>9. </b>Setup Frontend
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/5-workshop/10-using-chatbot/ title="Using Chatbot" class=dd-item><a href=../../5-workshop/10-using-chatbot/><b>10. </b>Using Chatbot
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/5-workshop/11-admin-dashboard/ title="Using Admin Dashboard" class=dd-item><a href=../../5-workshop/11-admin-dashboard/><b>11. </b>Using Admin Dashboard
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/5-workshop/12-cleanup/ title="Cleanup Resources" class=dd-item><a href=../../5-workshop/12-cleanup/><b>12. </b>Cleanup Resources
<i class="fas fa-check read-icon"></i></a></li></ul></li><li data-nav-id=/6-self-evaluation/ title=Self-Assessment class=dd-item><a href=../../6-self-evaluation/><b>6. </b>Self-Assessment
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/7-feedback/ title="Sharing and Feedback" class=dd-item><a href=../../7-feedback/><b>7. </b>Sharing and Feedback
<i class="fas fa-check read-icon"></i></a></li></ul><section id=shortcuts><h3>More</h3><ul><li><a class=padding href=https://www.facebook.com/groups/awsstudygroupfcj/><i class='fab fa-facebook'></i> AWS Study Group</a></li></ul></section><section id=prefooter><hr><ul><li><a class=padding><i class="fas fa-language fa-fw"></i><div class=select-style><select id=select-language onchange="location=this.value"><option id=en value=https://crystaljohn.github.io/fcj-workshop/3-blogstranslated/3.2-blog2/ selected>English</option><option id=vi value=https://crystaljohn.github.io/fcj-workshop/vi/3-blogstranslated/3.2-blog2/>Tiếng Việt</option></select><svg id="Capa_1" xmlns:xlink="http://www.w3.org/1999/xlink" width="255" height="255" viewBox="0 0 255 255" style="enable-background:new 0 0 255 255"><g><g id="arrow-drop-down"><polygon points="0,63.75 127.5,191.25 255,63.75"/></g></g></svg></div></a></li><li><a class=padding href=# data-clear-history-toggle><i class="fas fa-history fa-fw"></i> Clear History</a></li></ul></section><section id=footer><left><b>Workshop</b><br><img src="https://hitwebcounter.com/counter/counter.php?page=7920860&style=0038&nbdigits=9&type=page&initCount=0" title=Migrate alt="web counter" border=0></a><br><b><a href=https://cloudjourney.awsstudygroup.com/>Cloud Journey</a></b><br><img src="https://hitwebcounter.com/counter/counter.php?page=7830807&style=0038&nbdigits=9&type=page&initCount=0" title="Total CLoud Journey" alt="web counter" border=0>
</left><left><br><br><b>Last Updated</b><br><i><span id=lastUpdated style=color:orange></span>
</i><script>const today=new Date,formattedDate=today.toLocaleDateString("en-GB");document.getElementById("lastUpdated").textContent=formattedDate</script></left><left><br><br><b>Team</b><br><i><a href=https://www.facebook.com/groups/660548818043427 style=color:orange>First Cloud Journey</a><br></i></left><script async defer src=https://buttons.github.io/buttons.js></script></section></div></nav><section id=body><div id=overlay></div><div class="padding highlightable"><div><div id=top-bar><div id=breadcrumbs itemscope itemtype=http://data-vocabulary.org/Breadcrumb><span id=sidebar-toggle-span><a href=# id=sidebar-toggle data-sidebar-toggle><i class="fas fa-bars"></i>
</a></span><span id=toc-menu><i class="fas fa-list-alt"></i></span>
<span class=links><a href=../../>Internship Report</a> > <a href=../../3-blogstranslated/>Translated Blogs</a> > Build end-to-end Apache Spark pipelines with Amazon MWAA, Batch Processing Gateway, and Amazon EMR on EKS</span></div><div class=progress><div class=wrapper><nav id=TableOfContents><ul><li><a href=#by-avinash-desireddy-and-suvojit-dasgupta>By: Avinash Desireddy and Suvojit Dasgupta</a></li><li><a href=#solution-overview>Solution overview</a></li><li><a href=#airflow-custom-operator-for-bpg>Airflow custom operator for BPG</a></li><li><a href=#deploying-the-solution>Deploying the solution</a><ul><li><a href=#prerequisites>Prerequisites</a></li><li><a href=#common-infrastructure-setup>Common infrastructure setup</a></li><li><a href=#batch-processing-gateway-setup>Batch Processing Gateway setup</a></li><li><a href=#configuring-the-airflow-operator-for-bpg-on-amazon-mwaa>Configuring the Airflow operator for BPG on Amazon MWAA</a></li><li><a href=#configuring-airflow-connections-for-bpg-integration>Configuring Airflow connections for BPG integration</a></li><li><a href=#configuring-airflow-dag-to-run-spark-jobs>Configuring Airflow DAG to run Spark jobs</a></li><li><a href=#triggering-the-amazon-mwaa-dag>Triggering the Amazon MWAA DAG</a></li></ul></li><li><a href=#migrating-existing-airflow-dags-to-use-bpg>Migrating existing Airflow DAGs to use BPG</a></li><li><a href=#cleanup>Cleanup</a></li><li><a href=#conclusion>Conclusion</a></li><li><a href=#about-the-authors>About the authors</a></li></ul></nav></div></div></div></div><div id=head-tags></div><div id=body-inner><h1>Build end-to-end Apache Spark pipelines with Amazon MWAA, Batch Processing Gateway, and Amazon EMR on EKS</h1><h2 id=by-avinash-desireddy-and-suvojit-dasgupta>By: Avinash Desireddy and Suvojit Dasgupta</h2><p><strong>Date:</strong> May 1, 2025</p><p><strong>Topics:</strong> <a href=https://aws.amazon.com/emr/features/eks/>Amazon EMR on EKS</a>, <a href=https://aws.amazon.com/managed-workflows-for-apache-airflow/>Amazon Managed Workflows for Apache Airflow (Amazon MWAA)</a>, <a href=https://aws.amazon.com/big-data/>AWS Big Data</a>, <a href=https://aws.amazon.com/blogs/big-data/category/learning-level/intermediate-200/>Intermediate Level (200)</a>, <a href=https://aws.amazon.com/opensource/>Open Source</a></p><hr><p><a href=https://spark.apache.org/>Apache Spark</a> workloads running on <a href=https://aws.amazon.com/emr/features/emr-on-eks/>Amazon EMR on EKS</a> are foundational to many modern data platforms. EMR on EKS provides benefits by offering a managed Spark environment, integrating seamlessly with other AWS services, and your organization&rsquo;s existing Kubernetes-based deployment patterns.</p><p>Data platforms processing large-scale data volumes often require multiple EMR on EKS clusters. In the post <a href=https://aws.amazon.com/blogs/big-data/use-batch-processing-gateway-to-automate-job-management-in-multi-cluster-amazon-emr-on-eks-environments/>Use Batch Processing Gateway to automate job management in multi-cluster Amazon EMR on EKS environments</a>, we introduced <strong>Batch Processing Gateway (BPG)</strong> as a solution for managing Spark workloads across these clusters. Although BPG provides foundational functionality for workload distribution and routing support for Spark jobs in multi-cluster environments, enterprise data platforms demand additional features for a comprehensive data processing pipeline.</p><p>This post shows how to enhance the multi-cluster solution by integrating <a href=https://aws.amazon.com/mwaa/>Amazon Managed Workflows for Apache Airflow</a> (Amazon MWAA) with BPG. By using Amazon MWAA, we add job scheduling and orchestration capabilities, enabling you to build a comprehensive end-to-end Spark-based data processing pipeline.</p><hr><h2 id=solution-overview>Solution overview</h2><p>Consider HealthTech Analytics, a healthcare analytics company managing two distinct data processing workloads. Their Clinical Insights Data Science team processes sensitive patient outcome data, requiring HIPAA compliance and dedicated resources. Meanwhile, the Digital Analytics team handles website interaction data with more flexible requirements. As their operations have grown, they face increasing challenges in managing these diverse workloads efficiently.</p><p>The company needs to maintain strict separation between processing protected health information (PHI) and non-PHI data, while addressing different cost center requirements. The Clinical Insights team runs critical end-of-day batch processes that need guaranteed resources, whereas the Digital Analytics team can use cost-optimized spot instances for their variable workloads. Additionally, data scientists from both teams require environments for experimentation and prototyping on demand.</p><p>This scenario is an ideal use case for implementing a data pipeline using Amazon MWAA, BPG, and multiple EMR on EKS clusters. The solution needs to route different Spark workloads to appropriate clusters based on security requirements and cost profiles, while maintaining necessary isolation and compliance controls. To efficiently manage such an environment, we need a solution that maintains clear separation between application and infrastructure management concerns, and connects multiple components together into a robust pipeline.</p><p>Our solution involves integrating Amazon MWAA with BPG through an <a href=https://airflow.apache.org/docs/apache-airflow/stable/howto/custom-operator.html>Airflow custom operator</a> for BPG called the <strong>BPGOperator</strong>. This operator encapsulates the infrastructure management logic required to interact with BPG. The <code>BPGOperator</code> provides a clean interface for job submission through Amazon MWAA. When invoked, the operator communicates with BPG, which then routes Spark workloads to available EMR on EKS clusters based on predefined routing rules.</p><p>The following architecture diagram illustrates the components and their interactions.</p><p><img alt="Solution architecture" src=../../images/3-BlogsTranslated/3.2-Blog2/bdb-5005-architecture.png></p><p>The solution works through the following steps:</p><ol><li><strong>Amazon MWAA</strong> runs scheduled <a href=https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/dags.html>DAGs</a> using the <code>BPGOperator</code>. Data engineers create DAGs using this operator, requiring only the Spark application configuration file and basic scheduling parameters.</li><li><strong><code>BPGOperator</code></strong> validates and submits jobs to the BPG endpoint <code>POST:/apiv2/spark</code>. It handles all HTTP communication details, manages authentication tokens, and provides secure transmission of job configurations.</li><li><strong>BPG</strong> routes submitted jobs to EMR on EKS clusters based on predefined routing rules. These rules are centrally managed through BPG&rsquo;s configuration, enabling rule-based workload distribution across multiple clusters.</li><li><strong><code>BPGOperator</code></strong> monitors job status, logs, and handles execution retries. It polls the BPG job status endpoint <code>GET:/apiv2/spark/{subID}/status</code> and streams logs to Airflow by polling the <code>GET:/apiv2/log</code> endpoint every second. The BPG log endpoint retrieves the latest log information directly from the Spark Driver Pod.</li><li>DAG execution proceeds to subsequent tasks based on job completion status and defined dependencies. The <code>BPGOperator</code> communicates job status through Airflow&rsquo;s built-in task communication system, enabling complex workflow orchestration.</li></ol><p>Refer to BPG&rsquo;s <a href=https://github.com/aws-samples/amazon-emr-on-eks-batch-processing-gateway/blob/main/docs/api-spec/openapi.yaml>REST API interface</a> documentation for more details.</p><p>This architecture provides several key benefits:</p><ul><li><strong>Separation of concerns</strong> – Data Engineering and Platform Engineering teams in enterprise organizations often maintain distinct responsibilities. The modular design in this solution allows platform engineers to configure the <code>BPGOperator</code> and manage EMR on EKS clusters, while data engineers maintain DAGs.</li><li><strong>Centralized code management</strong> – The <code>BPGOperator</code> encapsulates all core functionality needed for Amazon MWAA DAGs to submit Spark jobs through BPG into a single, reusable Python module. This centralization minimizes code duplication across DAGs and improves maintainability by providing a standardized interface for job submission.</li></ul><hr><h2 id=airflow-custom-operator-for-bpg>Airflow custom operator for BPG</h2><p>An <a href=https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/operators.html>Operator</a> in Airflow is a template for a predefined <a href=https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/tasks.html>Task</a> that you can define declaratively inside your DAG. Airflow provides many built-in operators such as <code>BashOperator</code> to run bash commands, <code>PythonOperator</code> to run Python functions, and <code>EmrContainerOperator</code> to submit a new job to an EMR on EKS cluster. However, there is no built-in operator that performs all the steps required for Amazon MWAA integration with BPG.</p><p>Airflow allows you to create new operators to fit your specific requirements. This type of operator is called a <strong>custom operator</strong>. A custom operator encapsulates custom infrastructure-related logic into a single, maintainable component. Custom operators are created by extending the <code>airflow.models.baseoperator.BaseOperator</code> class. We developed and open-sourced an Airflow custom operator for BPG called <strong><code>BPGOperator</code></strong>, which performs the steps required to provide seamless integration of Amazon MWAA with BPG.</p><p>The following class diagram provides a detailed view of the <code>BPGOperator</code> implementation.</p><p><img alt="BPGOperator class diagram" src=../../images/3-BlogsTranslated/3.2-Blog2/bdb-5005-bpg-operator-class-diagram.png></p><p>When a DAG includes a <code>BPGOperator</code> task, the Amazon MWAA instance triggers the operator to submit a job request to BPG. The operator typically performs the following steps:</p><ol><li><strong>Job initialization</strong> – The <code>BPGOperator</code> prepares the job payload, including input parameters, configuration, connection details, and other metadata that BPG requires.</li><li><strong>Job submission</strong> – The <code>BPGOperator</code> handles HTTP POST requests to submit jobs to BPG endpoints with the provided configurations.</li><li><strong>Job execution monitoring</strong> – The <code>BPGOperator</code> checks the job status, polling BPG until the job completes successfully or fails. The monitoring process includes handling various job states, managing timeout scenarios, and responding to errors that occur during job execution.</li><li><strong>Job completion handling</strong> – Upon completion, the <code>BPGOperator</code> logs the job results, records relevant details, and can trigger subsequent tasks based on execution results.</li></ol><p>The following sequence diagram illustrates the interaction flow between the Airflow DAG, <code>BPGOperator</code>, and BPG.</p><p><img alt="Sequence diagram" src=../../images/3-BlogsTranslated/3.2-Blog2/bdb-5005-bgp-operator-sequence-diagram.png></p><hr><h2 id=deploying-the-solution>Deploying the solution</h2><p>In the remainder of this post, you will deploy an end-to-end pipeline to run Spark jobs across multiple EMR on EKS clusters. You will start by deploying the common components that form the foundation for building pipelines. Next, you will deploy and configure BPG on an EKS cluster, followed by deploying and configuring the <code>BPGOperator</code> on Amazon MWAA. Finally, you will run Spark jobs across multiple EMR on EKS clusters from Amazon MWAA.</p><p>To streamline the installation process, we have automated the deployment of all infrastructure components required for this post, so you can focus on the essential aspects of job submission to build an end-to-end pipeline. We provide detailed information to help you understand each step, simplifying setup while maintaining the learning experience.</p><p>To demonstrate the solution, you will create three clusters and one Amazon MWAA environment:</p><ul><li>Two EMR on EKS clusters: <code>analytics-cluster</code> and <code>datascience-cluster</code></li><li>One EKS cluster: <code>gateway-cluster</code></li><li>One Amazon MWAA environment: <code>airflow-environment</code></li></ul><p>The <code>analytics-cluster</code> and <code>datascience-cluster</code> serve as data processing clusters running Spark workloads, <code>gateway-cluster</code> hosts BPG, and <code>airflow-environment</code> hosts Airflow for job orchestration and scheduling.</p><p>You can find the source code in the <a href=https://github.com/aws-samples/sample-mwaa-bpg-emr-on-eks-spark-pipeline>GitHub repository</a>.</p><h3 id=prerequisites>Prerequisites</h3><p>Before deploying this solution, make sure the following prerequisites are met:</p><ul><li>Access to a valid AWS account</li><li><a href=https://aws.amazon.com/cli/>AWS Command Line Interface</a> (AWS CLI) <a href=https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html>installed</a> on your local machine</li><li>Git, <a href=https://docs.docker.com/get-docker/>Docker</a>, <a href=https://eksctl.io/>eksctl</a>, <a href=https://kubernetes.io/docs/tasks/tools/>kubectl</a>, <a href=https://helm.sh/docs/intro/install/>Helm</a>, and <a href=https://stedolan.github.io/jq/download/>jq</a> utilities installed on your local machine</li><li>Permissions to create AWS resources</li><li>Familiarity with Kubernetes, Amazon MWAA, Apache Spark, <a href=https://aws.amazon.com/eks/>Amazon Elastic Kubernetes Service</a> (Amazon EKS), and Amazon EMR on EKS</li></ul><h3 id=common-infrastructure-setup>Common infrastructure setup</h3><p>This step handles network infrastructure setup, including virtual private cloud (VPC) and subnets, along with configuring <a href=https://aws.amazon.com/iam/>AWS Identity and Access Management</a> (IAM) roles, <a href=https://aws.amazon.com/s3/>Amazon Simple Storage Service</a> (Amazon S3) storage, <a href=https://aws.amazon.com/ecr/>Amazon Elastic Container Registry</a> (Amazon ECR) repositories for BPG images, <a href=https://aws.amazon.com/rds/aurora/postgresql-features/>Amazon Aurora PostgreSQL-Compatible Edition</a> database, Amazon MWAA environment, and both EKS and EMR on EKS clusters with a preconfigured Spark operator. With this infrastructure provisioned automatically, you can focus on the subsequent steps without being bogged down by basic setup tasks.</p><p>Clone the repository to your local machine and set two environment variables. Replace <code>&lt;AWS_REGION></code> with the AWS Region where you want to deploy these resources.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>git clone https://github.com/aws-samples/sample-mwaa-bpg-emr-on-eks-spark-pipeline.git
</span></span><span style=display:flex><span>cd sample-mwaa-bpg-emr-on-eks-spark-pipeline
</span></span><span style=display:flex><span>export REPO_DIR<span style=color:#f92672>=</span><span style=color:#66d9ef>$(</span>pwd<span style=color:#66d9ef>)</span>
</span></span><span style=display:flex><span>export AWS_REGION<span style=color:#f92672>=</span>&lt;AWS_REGION&gt;
</span></span></code></pre></div><p>Run the following script to create the common infrastructure:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>cd <span style=color:#e6db74>${</span>REPO_DIR<span style=color:#e6db74>}</span>/infra
</span></span><span style=display:flex><span>./setup.sh
</span></span></code></pre></div><p>To verify successful infrastructure deployment, navigate to the <a href=https://console.aws.amazon.com/cloudformation/home>AWS CloudFormation console</a>, open your stack, and check the <strong>Events</strong>, <strong>Resources</strong>, and <strong>Outputs</strong> tabs to see the completion status, details, and list of created resources.</p><p>You have completed setting up the common components that form the foundation for the rest of the deployment.</p><h3 id=batch-processing-gateway-setup>Batch Processing Gateway setup</h3><p>This section builds the Docker image for BPG, deploys the helm chart on the EKS cluster <code>gateway-cluster</code>, and exposes the BPG endpoint using a Kubernetes service of type <code>LoadBalancer</code>. Complete the following steps:</p><ol><li>Deploy BPG on the EKS cluster <code>gateway-cluster</code>:</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>cd <span style=color:#e6db74>${</span>REPO_DIR<span style=color:#e6db74>}</span>/infra/bpg
</span></span><span style=display:flex><span>./configure_bpg.sh
</span></span></code></pre></div><ol start=2><li>Verify the deployment by listing pods and viewing the pod logs:</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl get pods --namespace bpg
</span></span><span style=display:flex><span>kubectl logs &lt;BPG-PODNAME&gt; --namespace bpg
</span></span></code></pre></div><p>Review the logs and confirm there are no errors or exceptions.</p><ol start=3><li>Exec into the BPG pod and verify the health check:</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl exec -it &lt;BPG-PODNAME&gt; -n bpg -- bash
</span></span><span style=display:flex><span>curl -u admin:admin localhost:8080/skatev2/healthcheck/status
</span></span></code></pre></div><p>The <code>healthcheck</code> API will return a successful response of <code>{"status":"OK"}</code>, confirming successful BPG deployment on the EKS cluster <code>gateway-cluster</code>.</p><p>We have successfully configured BPG on the <code>gateway-cluster</code> and set up EMR on EKS for both the <code>datascience-cluster</code> and <code>analytics-cluster</code>. This is the point where we stopped in the previous blog. In the following steps, we will configure Amazon MWAA with the <code>BPGOperator</code>, then write and submit DAGs to illustrate an end-to-end Spark-based data pipeline.</p><h3 id=configuring-the-airflow-operator-for-bpg-on-amazon-mwaa>Configuring the Airflow operator for BPG on Amazon MWAA</h3><p>This section configures the <code>BPGOperator</code> plugin on the Amazon MWAA environment <code>airflow-environment</code>.</p><ol><li>Configure the <code>BPGOperator</code> on Amazon MWAA:</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>cd <span style=color:#e6db74>${</span>REPO_DIR<span style=color:#e6db74>}</span>/bpg_operator
</span></span><span style=display:flex><span>./configure_bpg_operator.sh
</span></span></code></pre></div><ol start=2><li>On the Amazon MWAA console, navigate to the <code>airflow-environment</code> environment.</li><li>Choose <strong>Open Airflow UI</strong>, and in the Airflow interface, choose the <strong>Admin</strong> dropdown menu and choose <strong>Plugins</strong>.</li><li>You will see the <code>BPGOperator</code> plugin listed in the Airflow interface.</li></ol><p><img alt="BPGOperator Plugin" src=../../images/3-BlogsTranslated/3.2-Blog2/bdb-5005-mwaa-plugin.png></p><h3 id=configuring-airflow-connections-for-bpg-integration>Configuring Airflow connections for BPG integration</h3><p>This section walks you through setting up Airflow connections that enable secure communication between the Amazon MWAA environment and BPG. The <code>BPGOperator</code> uses the configured connection to authenticate and interact with BPG endpoints.</p><p>Run the following script to configure the Airflow connection <code>bpg_connection</code>:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>cd $REPO_DIR/airflow
</span></span><span style=display:flex><span>./configure_connections.sh
</span></span></code></pre></div><p>In the Airflow interface, choose the <strong>Admin</strong> dropdown menu and choose <strong>Connections</strong>. You will see <code>bpg_connection</code> listed.</p><p><img alt="Airflow Connections" src=../../images/3-BlogsTranslated/3.2-Blog2/bdb-5005-airflow-connection.png></p><h3 id=configuring-airflow-dag-to-run-spark-jobs>Configuring Airflow DAG to run Spark jobs</h3><p>This step configures an Airflow DAG to run a sample application. Specifically, we will submit a DAG containing multiple sample Spark jobs using Amazon MWAA to EMR on EKS clusters using BPG. Please wait a few minutes for the DAG to appear in the Airflow interface.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>cd $REPO_DIR/jobs
</span></span><span style=display:flex><span>./configure_job.sh
</span></span></code></pre></div><h3 id=triggering-the-amazon-mwaa-dag>Triggering the Amazon MWAA DAG</h3><p>In this step, we trigger the Airflow DAG and observe the job execution behavior, including reviewing the Spark logs in the Airflow interface:</p><ol><li>In the Airflow interface, review the DAG <code>MWAASparkPipelineDemoJob</code> and choose the play icon to trigger the DAG.</li></ol><p><img alt="Trigger DAG" src=../../images/3-BlogsTranslated/3.2-Blog2/bdb-5005-dags.png></p><ol start=2><li>Wait for the DAG to complete successfully.</li><li>When the DAG completes successfully, you will see <code>Success:1</code> under the <strong>Runs</strong> column.</li><li>In the Airflow interface, find and choose the DAG <code>MWAASparkPipelineDemoJob</code>.</li></ol><p><img alt="Successful DAG" src=../../images/3-BlogsTranslated/3.2-Blog2/bdb-5005-dag-graphview.png></p><ol start=5><li>On the <strong>Graph</strong> tab, choose any task (for example, we choose the task <code>calculate_pi</code>) and then choose <strong>Logs</strong>.</li></ol><p><img alt="View Logs" src=../../images/3-BlogsTranslated/3.2-Blog2/bdb-5005-mwaa-job-logs.png></p><ol start=6><li>View the Spark logs in the Airflow interface.</li></ol><hr><h2 id=migrating-existing-airflow-dags-to-use-bpg>Migrating existing Airflow DAGs to use BPG</h2><p>In enterprise data platforms, a typical data pipeline consists of Amazon MWAA submitting Spark jobs to multiple EMR on EKS clusters using the <code>SparkKubernetesOperator</code> and an <code>Airflow Connection</code> of type Kubernetes. An Airflow Connection is a set of parameters and credentials used to establish communication between Amazon MWAA and external systems or services. A DAG references the connection name and connects to the external system.</p><p>The following diagram shows the typical architecture.</p><p><img alt="Typical architecture" src=../../images/3-BlogsTranslated/3.2-Blog2/bdb-5005-mwaa-existing.png></p><p>In this setup, Airflow DAGs typically use <code>SparkKubernetesOperator</code> and <code>SparkKubernetesSensor</code> to submit Spark jobs to a remote EMR on EKS cluster using <code>kubernetes_conn_id=&lt;connection_name></code>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Submit Spark-Pi job using Kubernetes connection</span>
</span></span><span style=display:flex><span>submit_spark_pi <span style=color:#f92672>=</span> SparkKubernetesOperator(
</span></span><span style=display:flex><span>    task_id<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;submit_spark_pi&#39;</span>,
</span></span><span style=display:flex><span>    namespace<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;default&#39;</span>,
</span></span><span style=display:flex><span>    application_file<span style=color:#f92672>=</span>spark_pi_yaml,
</span></span><span style=display:flex><span>    kubernetes_conn_id<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;emr_on_eks_connection_[1|2]&#39;</span>,  <span style=color:#75715e># Connection ID defined in Airflow</span>
</span></span><span style=display:flex><span>    dag<span style=color:#f92672>=</span>dag)
</span></span></code></pre></div><p>To migrate the infrastructure to BPG-based infrastructure without affecting environment continuity, we can deploy a parallel infrastructure using BPG, create a new Airflow Connection for BPG, and gradually migrate DAGs to use the new connection. By doing so, we will not disrupt the existing infrastructure until the BPG-based infrastructure is fully operational, including migrating all existing DAGs.</p><p>The following diagram introduces the transient state where both Kubernetes connections and BPG connections are operational. Blue arrows indicate existing workflow paths, and red arrows represent new BPG-based migration paths.</p><p><img alt="Migration state" src=../../images/3-BlogsTranslated/3.2-Blog2/bdb-5005-mwaa-bpg-migration.png></p><p>The modified code snippet for the DAG is as follows:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Submit Spark-Pi job using BPG connection</span>
</span></span><span style=display:flex><span>submit_spark_pi <span style=color:#f92672>=</span> BPGOperator(
</span></span><span style=display:flex><span>    task_id<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;submit_spark_pi&#39;</span>,
</span></span><span style=display:flex><span>    application_file<span style=color:#f92672>=</span>spark_pi_yaml,
</span></span><span style=display:flex><span>    application_file_type<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;yaml&#39;</span>,
</span></span><span style=display:flex><span>    connection_id<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;bpg_connection&#39;</span>,  <span style=color:#75715e># Connection ID defined in Airflow</span>
</span></span><span style=display:flex><span>    dag<span style=color:#f92672>=</span>dag)
</span></span></code></pre></div><p>Finally, when all DAGs have been modified to use <code>BPGOperator</code> instead of <code>SparkKubernetesOperator</code>, you can decommission any remnants of the old workflow. The final state of the infrastructure will look like the following diagram.</p><p><img alt="Final state" src=../../images/3-BlogsTranslated/3.2-Blog2/bdb-5005-mwaa-bpg-final-1.png></p><p>Using this approach, we can seamlessly integrate BPG into an environment that currently only uses Amazon MWAA and EMR on EKS clusters.</p><hr><h2 id=cleanup>Cleanup</h2><p>To avoid incurring future charges from resources created in this walkthrough, clean up your environment after you have completed the steps. You can do this by running the <code>cleanup.sh</code> script, which will safely delete all resources provisioned during setup:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>cd <span style=color:#e6db74>${</span>REPO_DIR<span style=color:#e6db74>}</span>/setup
</span></span><span style=display:flex><span>./cleanup.sh
</span></span></code></pre></div><hr><h2 id=conclusion>Conclusion</h2><p>In the post <a href=https://aws.amazon.com/blogs/big-data/use-batch-processing-gateway-to-automate-job-management-in-multi-cluster-amazon-emr-on-eks-environments/>Use Batch Processing Gateway to automate job management in multi-cluster Amazon EMR on EKS environments</a>, we introduced Batch Processing Gateway as a solution for routing Spark workloads across multiple EMR on EKS clusters. In this post, we demonstrated how to enhance this foundation by integrating BPG with Amazon MWAA. Through our custom <code>BPGOperator</code>, we showed how to build robust end-to-end Spark-based data processing pipelines while maintaining clear separation of concerns and centralized code management. Finally, we demonstrated how to seamlessly integrate the solution into your existing Amazon MWAA and EMR on EKS data platform without affecting operational continuity.</p><p>We encourage you to experiment with this architecture in your own environment, adapting it to fit your unique workloads and operational requirements. By deploying this solution, you can build efficient and scalable data processing pipelines, harnessing the full potential of EMR on EKS and Amazon MWAA. Explore further by deploying the solution in your AWS account while adhering to your organization&rsquo;s security best practices and share your experiences with the AWS Big Data community.</p><hr><h2 id=about-the-authors>About the authors</h2><p><strong>Suvojit Dasgupta</strong> is a Principal Data Architect at AWS. He leads a team of skilled engineers in designing and building scalable data solutions for AWS customers. He specializes in developing and deploying innovative data architectures to solve complex business challenges.</p><p><strong>Avinash Desireddy</strong> is a Cloud Infrastructure Architect at AWS, passionate about building secure data applications and platforms. He has deep experience in Kubernetes, DevOps, and enterprise architecture, helping customers containerize applications, streamline deployments, and optimize cloud-native environments.</p><footer class=footline></footer></div></div><div id=navigation><a class="nav nav-prev" href=../../3-blogstranslated/3.1-blog1/ title="Build a Virtual Meteorologist Using Amazon Bedrock Agents"><i class="fa fa-chevron-left"></i></a>
<a class="nav nav-next" href=../../3-blogstranslated/3.3-blog3/ title="Building Golden Images with CIS Linux Build Kit in Amazon EC2 Image Builder" style=margin-right:0><i class="fa fa-chevron-right"></i></a></div></section><div style=left:-1000px;overflow:scroll;position:absolute;top:-1000px;border:none;box-sizing:content-box;height:200px;margin:0;padding:0;width:200px><div style=border:none;box-sizing:content-box;height:200px;margin:0;padding:0;width:200px></div></div><script src=../../js/clipboard.min.js?1765512673></script><script src=../../js/perfect-scrollbar.min.js?1765512673></script><script src=../../js/perfect-scrollbar.jquery.min.js?1765512673></script><script src=../../js/jquery.sticky.js?1765512673></script><script src=../../js/featherlight.min.js?1765512673></script><script src=../../js/highlight.pack.js?1765512673></script><script>hljs.initHighlightingOnLoad()</script><script src=../../js/modernizr.custom-3.6.0.js?1765512673></script><script src=../../js/learn.js?1765512673></script><script src=../../js/hugo-learn.js?1765512673></script><link href=../../mermaid/mermaid.css?1765512673 rel=stylesheet><script src=../../mermaid/mermaid.js?1765512673></script><script>mermaid.initialize({startOnLoad:!0})</script><script>(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,(e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date),(i=t.createElement(n),a=t.getElementsByTagName(n)[0]),i.async=1,i.src=s,a.parentNode.insertBefore(i,a)})(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-158079754-2","auto"),ga("send","pageview")</script></body></html>