[{"uri":"https://crystaljohn.github.io/fcj-workshop/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Trần Phan Thanh Phúc\nPhone Number: 0944941764\nEmail: phuctptse183992@fpt.edu.vn\nUniversity: FPT University\nMajor: Software Engineering\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd\nInternship Position: FCJ Cloud Intern\nInternship Period: From 12/08/2025 to 12/11/2025\nReport Contents Worklog Proposal Translated Blogs Participated Events Workshop Self-evaluation Feedback "},{"uri":"https://crystaljohn.github.io/fcj-workshop/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Objectives: Get familiar with SageMaker Studio and hands-on workshop exercises. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn about Qdrant on EC2 20/10/2025 20/10/2025 https://aws.amazon.com/marketplace/pp/prodview-rtphb42tydtzg 3 - Learn how to set up with Amazon Bedrock APIs 21/10/2025 21/10/2025 https://github.com/aws-samples/amazon-bedrock-samples/tree/main/introduction-to-bedrock/bedrock_apis 4 - Create a SageMaker domain to test Amazon Bedrock in SageMaker - Set up Bedrock and prompts with Claude 3.5 Sonnet - How to work with Amazon Bedrock APIs 22/10/2025 22/10/2025 https://cloudjourney.awsstudygroup.com/ 5 - Practice: Install Bedrock Claude 3.5 Sonnet Basic API invoke Batch processing Prompt engineering Rate limit handling 23/10/2025 23/10/2025 https://cloudjourney.awsstudygroup.com/ 6 - Learn concepts: Amazon Simple Queue Service (SQS) Amazon Simple Notification Service (SNS) AWS Organizations Amazon Macie Direct Connect 24/10/2025 24/10/2025 https://skillbuilder.aws/learn/94T2BEN85A/aws-cloud-practitioner-essentials/ Week 7 Achievements: Understood Qdrant on EC2 as a self-hosted vector database: how it stores and searches embeddings and how it compares cost-wise to managed options (e.g., OpenSearch Serverless ~$90–150/month).\nUsed Qdrant to:\nStore embeddings generated by Titan Text Embeddings v2 after chunking PDF/OCR outputs. Retrieve top-K vectors for queries and feed them into Claude Sonnet for RAG. Created a SageMaker domain and validated Bedrock connectivity:\nSet up a simple SageMaker domain for notebook-based experiments calling Bedrock APIs. Verified the end-to-end flow: send prompt → receive response from Claude 3.5 Sonnet. Implemented and tested Amazon Bedrock API calls:\nWrote sample scripts for sync/async calls, implemented retry/backoff and observed rate limits. Ran batch processing experiments (multiple prompt/embedding jobs) to observe quotas and throttling behavior. Prompt engineering \u0026amp; cost management:\nExperimented with Prompt Caching by separating System/Tool schema and Conversation History; observed reduced input tokens when reusing cache. Tuned prompts for Claude Sonnet to improve answer quality while reducing token usage. Messaging \u0026amp; observability:\nLearned the role of SQS for ingestion pipelines and SNS for alerts/notifications. Understood how CloudWatch + SNS can be used to trigger alerts when workers fail or SQS backlog grows. Security \u0026amp; infra basics:\nStudied AWS Organizations and IAM roles for managing accounts and access to Bedrock/Textract. Familiarized with Amazon Macie (data classification) and the use-case for Direct Connect in on-prem integrations. Practical takeaway \u0026amp; next steps:\nClarified the end-to-end flow: Upload PDF → SQS message → EC2 Worker (Textract → chunk → Titan Embeddings) → index into Qdrant → RAG with Claude Sonnet. Next plan: implement a sample worker on EC2 to automate ingestion workflows, measure performance, and optimize batch size/throughput. "},{"uri":"https://crystaljohn.github.io/fcj-workshop/1-worklog/","title":"Worklog","tags":[],"description":"","content":"This worklog covers 12 weeks of internship at First Cloud Journey, focusing on building ARC-Chatbot — a RAG Chatbot application on AWS.\nWeek 1: Getting familiar with AWS and basic services\nWeek 2: Getting familiar with AWS VPC and practicing labs\nWeek 3: Deploying EC2 instances in VPC Public/Private Subnets\nWeek 4: Deploying applications on Linux Instance\nWeek 5: Learning S3, IAM and Security best practices\nWeek 6: Studying Serverless: Lambda, API Gateway, DynamoDB\nWeek 7: Learning Containers: Docker, ECR, ECS\nWeek 8: Studying AI/ML Services: Bedrock, Textract\nWeek 9: Designing Architecture and planning ARC-Chatbot project\nWeek 10: Assessment (M0) — Setup AWS Account, Bedrock \u0026amp; Textract APIs, Architecture\nWeek 11: Setup Infrastructure (M0) \u0026amp; IDP Pipeline (M1) — S3, DynamoDB, Cognito, SQS, PDF Processing\nWeek 12: RAG Chat (M2) \u0026amp; Testing (M3) — Rate Limiting, Chat UI, Admin Dashboard\n"},{"uri":"https://crystaljohn.github.io/fcj-workshop/3-blogstranslated/3.1-blog1/","title":"Build a Virtual Meteorologist Using Amazon Bedrock Agents","tags":[],"description":"","content":"By: Salman Ahmed, Ankush Goyal, Sergio Barraza, and Ravi Kumar Date: 11 FEB 2025\nTopics: Amazon Bedrock, Amazon Bedrock Agents, Amazon Cognito, AWS Amplify, AWS Lambda, Generative AI, Intermediate (200)\nIntroduction The integration of generative AI capabilities is driving transformative changes across multiple industries. While weather information is accessible through various channels, businesses that rely heavily on meteorological data need robust and scalable solutions to efficiently manage and leverage these critical insights while reducing manual processes. The solution in this post demonstrates how to build an AI-powered virtual meteorologist that can answer complex weather-related queries using natural language. We use multiple AWS services to deploy a complete solution that you can use to interact with an API that provides real-time weather data. In this solution, we use Amazon Bedrock Agents.\nAmazon Bedrock Agents help streamline workflows and automate repetitive tasks. Amazon Bedrock Agents can securely connect to your enterprise data sources and supplement user requests with accurate responses. You can use Amazon Bedrock Agents to design an action schema that fits your requirements, thereby controlling whenever the agent initiates a specified action. This flexible approach allows you to integrate and execute business logic in your preferred backend service, creating a harmonious combination of functionality and flexibility. The system also has the ability to remember across interactions, providing a more personalized experience for users.\nIn this post, we introduce a streamlined approach to deploying an AI-powered agent by combining Amazon Bedrock Agents and a foundation model (FM). We guide you through the process of configuring the agent and implementing the specialized logic required for the virtual meteorologist to provide accurate weather-related responses. Additionally, we use multiple AWS services, including AWS Amplify for hosting the front end, AWS Lambda functions for processing request logic, Amazon Cognito for user authentication, and AWS Identity and Access Management (IAM) for controlling access to the agent.\nSolution Overview The following diagram provides an overall view and highlights the main components of the system. The architecture uses Amazon Cognito for user authentication and AWS Amplify as the hosting environment for the front-end application.\nAmazon Bedrock Agents receives queries from users and passes the information to action groups, each of which triggers a corresponding custom Lambda function. Each action group and Lambda function handles a specific task:\ngeo-coordinates – Processes geographic coordinate information to retrieve data about a specific location. weather – Collects weather information for the provided location. date-time – Determines the current date and time according to the appropriate time zone. The solution leverages AWS\u0026rsquo;s auto-scaling capabilities, built-in security, and serverless model to minimize operational costs while ensuring real-time responsiveness.\nPrerequisites To fully deploy this solution, you need to prepare the following components:\nAn active AWS account. Access to a foundation model (FM) in Amazon Bedrock, specifically Anthropic Claude 3.5 Sonnet, in the same Region where you will deploy the solution. Download the accompanying AWS CloudFormation template from the aws-samples GitHub repository to automatically provision resources. Deploy Resources Using AWS CloudFormation After preparing the prerequisites, you will deploy all necessary resources using AWS CloudFormation. When you run the CloudFormation template, the system will automatically create the following components (note that the AWS services used in this solution will incur costs):\nAmazon Cognito User pool: CognitoUserPoolforVirtualMeteorologistApp App client: VirtualMeteorologistApp Identity pool: cognito-identity-pool-vm AWS Lambda \u0026lt;Stack name\u0026gt;-geo-coordinates-\u0026lt;auto-generated\u0026gt; \u0026lt;Stack name\u0026gt;-weather-\u0026lt;auto-generated\u0026gt; \u0026lt;Stack name\u0026gt;-date-time-\u0026lt;auto-generated\u0026gt; Amazon Bedrock Agents Agent: virtual-meteorologist Action group (1): obtain-latitude-longitude-from-place-name Action group (2): obtain-weather-information-with-coordinates Action group (3): get-current-date-time-from-timezone After the CloudFormation stack is successfully deployed, you need to copy the values from the Outputs tab in the AWS CloudFormation Console to configure the application in AWS Amplify. The values to save include:\nAWSRegion BedrockAgentAliasId BedrockAgentId BedrockAgentName IdentityPoolId UserPoolClientId UserPoolId Deploy AWS Amplify Application The user interface application is stored in the AWS-Amplify-Frontend.zip file available on GitHub.\nYou need to follow these steps to manually deploy the application:\nDownload the AWS-Amplify-Frontend.zip source code file from the GitHub repository. Use this .zip file to manually deploy the application in AWS Amplify. After deployment is complete, return to the AWS Amplify Console page and use the automatically generated domain to access the web application. Use Amazon Cognito for User Authentication Amazon Cognito is an identity service that allows you to authenticate and authorize users. In this solution, we use Amazon Cognito to verify user identity before they can access and use the application. Additionally, we use Identity Pool to provide temporary AWS credentials to users during their interaction with the Amazon Bedrock API.\nUse Amazon Bedrock Agents to Automate Tasks in the Application With Amazon Bedrock Agents, you can build and configure automated agents in your application. An agent helps end users perform actions based on organizational data and information they provide. These agents orchestrate the entire interaction process between foundation models (FMs), data sources, application software, and user conversations. This allows the system to understand and execute complex requests in natural context while efficiently automating data processing workflows.\nUse Action Groups to Define Actions Amazon Bedrock Agent Performs An action group defines a set of related actions that an Amazon Bedrock Agent can perform to assist users. When configuring an action group, you have several options for processing user-provided data, including: adding user input to the agent\u0026rsquo;s action group, passing data to an AWS Lambda for custom business logic processing, or returning control directly to the application through the InvokeAgent method response.\nIn this application, we create three action groups that enable the Amazon Bedrock Agent to perform core functions: retrieve coordinates for a specific location, get the current date and time, and collect weather data at that location. These action groups allow the agent to access and process critical information, helping enhance the accuracy and comprehensive responsiveness to user queries related to geographic location and weather conditions.\nUse AWS Lambda for Amazon Bedrock Action Groups In this solution, three AWS Lambda functions are deployed to support the action groups defined by the Amazon Bedrock Agent:\nLocation Coordinates Lambda Function – This function is triggered by the obtain-latitude-longitude-from-place-name action group. It takes a place name as input and returns the corresponding geographic coordinates (latitude and longitude). The function uses a geocoding service or database to look up this information. Date and Time Lambda Function – This function is triggered by the get-current-date-time-from-timezone action group. It determines the time zone and returns current date and time information. Weather Information Lambda Function – This function is triggered by the obtain-weather-information-with-coordinates action group. It takes the coordinates provided from the first function and retrieves current weather data or forecasts for the corresponding area through a weather API. Each Lambda function receives an input event containing metadata and data fields populated from the Amazon Bedrock Agent\u0026rsquo;s API operation or function parameters. These functions process the data, execute their respective tasks, and return responses containing the necessary information. That response is then aggregated by the Amazon Bedrock Agent to form a complete answer for the user.\nBy using Lambda functions, the Amazon Bedrock Agent can access external data sources and perform complex calculations, significantly enhancing the agent\u0026rsquo;s capabilities in handling requests related to location, time, and weather conditions.\nUse AWS Amplify for Front-End Code AWS Amplify provides a development environment for secure, scalable web and mobile applications. Developers can focus on writing code instead of managing infrastructure. Amplify also integrates with multiple source code hosting platforms like GitHub, Bitbucket, or GitLab. In this solution, we manually upload the front-end source code to AWS Amplify, following the process described in the previous section of this post.\nExperience the Application After deploying the application in AWS Amplify, you can access the automatically generated URL to open the user interface. When accessing this link, the application will request information related to Amazon Cognito and Amazon Bedrock Agents. This information is necessary to securely authenticate users and allow the front end to interact with the agent on Bedrock. This enables the application to manage user sessions and make authorized API calls to AWS services on behalf of users.\nYou need to enter the information obtained from the CloudFormation Outputs section, including:\nUser Pool ID User Pool Client ID Identity Pool ID Region Agent Name Agent ID Agent Alias ID Sign in with your username and password. A temporary password was automatically generated during deployment and sent to the email you provided when running the CloudFormation template. On your first login, you will be asked to change your password to complete the authentication process. After successfully signing in, you can start asking questions to the application, for example:\n\u0026ldquo;Is today a good day to have a BBQ party in Dallas, Texas?\u0026rdquo; Within just a few seconds, the application will display a detailed answer indicating whether today is suitable for hosting a BBQ party in Dallas.\nSample Query Examples Below are some questions demonstrating the virtual meteorologist\u0026rsquo;s conversational capabilities:\n\u0026ldquo;What\u0026rsquo;s the weather like in New York City today?\u0026rdquo; \u0026ldquo;Should I plan an outdoor birthday party in Miami next weekend?\u0026rdquo; \u0026ldquo;Will there be snow in Denver on Christmas Day?\u0026rdquo; \u0026ldquo;Can I go swimming at the beach in Chicago today?\u0026rdquo; These queries demonstrate how the agent can provide current weather information, advise on activities based on forecasts, and predict future weather conditions. Users can ask questions related to specific activities (such as swimming, outdoor parties\u0026hellip;), and the system will automatically analyze weather conditions to provide appropriate recommendations.\nClean Up Resources If you no longer want to use the virtual meteorologist, follow these steps to delete the application and all related resources deployed using AWS CloudFormation and AWS Amplify:\n1. Delete CloudFormation Stack In the AWS CloudFormation Console, select Stacks in the navigation bar. Identify the stack you created during deployment (the stack name was set during initialization). Select that stack and click Delete to remove all related resources. 2. Delete Amplify Application and Associated Resources For detailed instructions, please refer to the Clean Up Resources section in the official AWS documentation.\nConclusion This solution demonstrates the power of combining Amazon Bedrock Agents with other AWS services to create an intelligent, conversational weather assistant. Through the application of AI and cloud computing technology, businesses can automate complex queries and provide users with practical, valuable information.\nAdditional Resources To learn more about Amazon Bedrock, you can refer to:\nGitHub repo: Amazon Bedrock Workshop Amazon Bedrock User Guide Workshop: Using Generative AI on AWS for diverse content types To learn more about the Anthropic Claude 3.5 Sonnet model, see:\nAnthropic\u0026rsquo;s Claude in Amazon Bedrock About the Authors Salman Ahmed is a Senior Technical Account Manager at AWS Enterprise Support. He assists customers in the travel and hospitality industry with designing, deploying, and operating cloud infrastructure. With a passion for networking services and many years of experience, he helps customers effectively adopt AWS Networking solutions. Outside of work, Salman enjoys photography, traveling, and sports.\nSergio Barraza is a Senior Enterprise Support Lead at AWS, helping energy industry customers design and optimize cloud solutions. With a passion for software development, he guides customers in leveraging AWS services. Outside of work, Sergio is a musician who plays multiple instruments including guitar, piano, drums, and practices Wing Chun Kung Fu.\nRavi Kumar is a Senior Technical Account Manager at AWS Enterprise Support, helping travel and hospitality industry customers optimize their operations on AWS. He has over 20 years of IT experience and always focuses on operational efficiency. Outside of work, Ravi enjoys painting, playing cricket, and exploring new places.\nAnkush Goyal is an Enterprise Support Lead at AWS Enterprise Support, helping customers simplify cloud operations. With over 20 years of experience, he focuses on optimizing infrastructure and delivering long-term value for businesses.\n"},{"uri":"https://crystaljohn.github.io/fcj-workshop/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":"Report: “BUILDING AGENTIC AI” Event Purpose Optimize context handling with Amazon Bedrock Build AI agent automation using Amazon Bedrock through practical techniques and real-world use cases Speakers Nguyen Gia Hung — Head of Solutions Architect, AWS Kien Nguyen — Solutions Architect, AWS Viet Pham — Founder \u0026amp; CEO, Diagflow Kha Van — Community Leader, AWS Thang Ton — Co-Founder \u0026amp; COO, Cloud Thinker Henry Bui — Head of Engineering, Cloud Thinker Key Content Cost and performance optimization techniques for AI agent systems Long product release cycles → lost revenue / missed opportunities Inefficient operations → lower productivity, higher cost Non-compliance with security regulations → security incidents, reputational loss Four \u0026ldquo;Quick Win\u0026rdquo; techniques for optimization Prompt Caching This was highlighted as the most important technique — it can reduce costs by 70–90% and speed up processing. Context structure: split the context window into three parts: (1) System \u0026amp; Tool Schema, (2) Conversation History, and (3) Objective Prompt. Common mistake: many teams only cache the System Prompt and Tool Schema, but Conversation History is often the most expensive part (80–90% of cost) and is neglected. Correct strategy: place checkpoints so the entire conversation history can be cached. The first run (cache write) may add ~25% extra cost, but subsequent runs save ~90%.\nContext Compaction (Summarization) Cloud Thinker presented an intelligent summarization pattern to avoid losing cache hits. Old approach: create a new agent to summarize past conversation — this invalidates the previous cache and often degrades quality. New approach (Cloud Thinker technique): keep the same agent and cached history, but swap the Objective Prompt to a task like \u0026ldquo;summarize this conversation.\u0026rdquo; This leverages cache hits, reduces summarization cost from ~$0.30 to ~$0.03 (≈90% savings) and improves output quality.\nTool Consolidation Problems with protocols like MCP (Model Context Protocol) arise when too many tools (e.g., 50 tools) are injected into context, causing context flooding. Solution: instead of embedding full complex schemas in the prompt, use a lightweight dictionary and consolidate instructions. Just-in-time instruction: agents can request detailed tool usage instructions only when needed (e.g., via a get instruction call), reducing tokens sent in each request.\nParallel Tool Calling Modern models allow calling multiple tools in parallel (instead of the older sequential ReAct pattern), saving time. This often requires explicit instructions (e.g., \u0026ldquo;maximize efficiency\u0026rdquo;) to force parallel execution.\nWhat I Learned Cost management strategies Input cost is a major part of operating AI agents in loops because each loop may re-send conversation history and system prompts. Longer histories increase input tokens and cost. Solution: use Prompt Caching and checkpointing to cut costs by up to 80–90%. Smart Summarization techniques Keep the same agent and cached history to preserve cache hits and quality. This approach reduced summarization cost from $0.30 to $0.03 in practice and produced better outputs. Tool Design: Avoid context flooding Problem: injecting too many tools (e.g., MCP with 50 tools) floods the context. Fix: provide a special instruction for agents to fetch detailed tool instructions on demand instead of embedding full schemas. Benefits: smaller context, lower token usage, better efficiency. Performance optimization: enforce Parallel Tool Calling Add explicit instructions to prompts so the model runs tasks in parallel and maximizes efficiency. Event Experience Attending the \u0026ldquo;Building Agentic AI\u0026rdquo; workshop was an engaging technical experience that broadened my understanding of agent design and prompt engineering. Notable experiences included:\nLearning from expert speakers Speakers from AWS, Cloud Thinker, and Diagflow shared practical best practices for designing modern agent-based applications. Hands-on technical exercises Participated in a CloudThinker hackathon focused on optimizing systems for cost and performance using CloudThinker tools. Exploring modern tools Hands-on exposure to CloudThinker and practical demonstrations of how to apply these techniques. Event photos Overall, the event not only delivered technical knowledge but also shifted my thinking about application design, agent workflows, and prompt optimization to achieve better results.\n"},{"uri":"https://crystaljohn.github.io/fcj-workshop/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":"Summary Report: \u0026ldquo;AWS Cloud Mastery Series #3 workshop\u0026rdquo; Event Objectives Share and reinforce core knowledge in AWS Well-Architected Framework – Security Pillar. Equip mindset and skills to build secure, sustainable systems that comply with AWS standards. Speakers FCJ Team Kha Van - Cloud Security Engineer Key Highlights Security Foundation – AWS Security Foundation Least Privilege – Zero Trust – Defense in Depth: 3 foundational principles for all modern security systems. Shared Responsibility Model: AWS secures the cloud, customers secure in the cloud. Top Cloud Threats in Vietnam Public S3 / DB / Redis Exposed access keys IAM misconfiguration EC2 infected with malware (mining) Missing logging or GuardDuty not enabled Pillar 1 — Identity \u0026amp; Access Management (IAM) IAM Users: almost no longer suitable — prioritize using Roles + SSO + OIDC. IAM Identity Center (SSO): used for multi-account management. Organizational permissions: apply SCP and permission boundaries to control permissions at the organizational level. Authentication security \u0026amp; monitoring: mandatory credential rotation, MFA; use Access Analyzer to detect overly broad policies. Pillar 2 — Detection \u0026amp; Monitoring CloudTrail (organization level): stores all API activity for audit and incident investigation. GuardDuty: detects anomalous behavior on EC2, IAM, S3, EKS. Security Hub: aggregates findings from multiple services into a central dashboard. Logging sources: VPC Flow Logs, S3 access logs, ALB logs — used to trace network \u0026amp; access patterns. Alerting \u0026amp; Automation: use EventBridge to send alerts and trigger automatic remediation. Detection-as-Code: manage detection rules as source code (IaC) for easier review and deployment. Case study: Without logs, there\u0026rsquo;s no evidence — incident investigation becomes impossible.\nPillar 3 — Infrastructure Protection VPC segmentation: reduce \u0026ldquo;blast radius\u0026rdquo; when systems are attacked by isolating network zones. Subnet placement (Public vs Private): Public: ALB, NAT Gateway. Private: EC2 (app), Databases, internal services. Security Group (SG) vs Network ACL (NACL): SG = stateful, applied at instance level. NACL = stateless, applied at subnet level. Edge protection: WAF, Shield Advanced, Network Firewall for edge defense layers. Workload hardening: best practices for EC2, ECS, EKS security (patching, minimal IAM, image scanning, runtime protections). Case study: Most security breaches stem from placing resources in the wrong public subnet — always check placement before deployment.\nPillar 4 — Data Protection Encryption everywhere: enable encryption for S3, EBS, RDS, DynamoDB (at-rest). AWS KMS: Manage Key policies, Grants, and Rotation for key lifecycle. Encryption layers: separate encryption at-rest and in-transit (TLS/HTTPS). Secrets management: use Secrets Manager / Parameter Store: Automatic secret rotation. Manage credentials according to AWS best practices. Data classification \u0026amp; guardrails: classify data by sensitivity level and apply guardrails (IAM, encryption, access controls). Case study: If you encrypt from the start, data breach risk can be reduced by ~50%.\nPillar 5 — Incident Response (IR) AWS IR lifecycle: Prepare → Detect → Investigate → Respond → Recover.\nPlaybooks (real examples):\nCompromised IAM key Public S3 bucket EC2 infected with malware / mining Tools \u0026amp; handling techniques:\nSnapshot (EBS/instance snapshot) to preserve evidence. Isolation by changing Security Group to disconnect instance. Evidence collection for forensic and audit. Incident Response automation:\nLambda – run small remediation, cleanup. Step Functions – orchestrate complex IR workflows. EventBridge – trigger playbooks when events are detected. Case study: Incident response cannot rely entirely on humans — automation is needed as much as possible.\nKey Takeaways Modern Security Mindset Zero Trust \u0026amp; Least Privilege: prioritize zero-trust model by default and restrict permissions to the minimum level. Defense in Depth: build multiple defense layers to reduce risk when one layer is breached. Traceability \u0026amp; IaC: all changes must be traceable and reproducible — deploy infrastructure with Infrastructure as Code (IaC). Don\u0026rsquo;t trust manual configuration: avoid manual configuration, prioritize coding and testing. Critical Technical Architecture IAM\nRoles \u0026gt; Users SSO \u0026gt; local credentials OIDC \u0026gt; Access Keys Network\nPrivate-first: always prioritize placing resources in private placement. Security Group (SG) is the \u0026ldquo;main wall\u0026rdquo; (stateful); NACL is supplementary layer (stateless). Outbound filtering is equally important as inbound filtering. Data\nEncrypt by default (S3, EBS, RDS, DynamoDB). Secret rotation according to policy. Zero exposure for S3/DB — restrict public access. Detection\nCloudTrail ON GuardDuty ON Comprehensive logging to support incident investigation. Incident Response (IR)\nClear playbook. Automation (remediation, rollback). Rollback / snapshot for quick recovery. Modernization Strategy No rush → Phased approach. Use multi-account architecture to reduce risk. Applying to Work Design project IAM with Roles + Permission Sets. Apply VPC segmentation and \u0026ldquo;private-first design\u0026rdquo;. Add GuardDuty + CloudTrail org-level for entire dev/prod environment. Set up Alerting \u0026amp; IR automation (EventBridge → Lambda). Apply Secret Manager + rotation → no hard-coded secrets. Deploy IaC (Terraform/CDK) to reduce drift. Event Experience Attending the \u0026ldquo;AWS Cloud Mastery Series #3\u0026rdquo; workshop was an extremely valuable experience, giving me a comprehensive view of how to build secure and sustainable systems using modern methods and tools. Some highlights: Learning from highly skilled speakers Understood in detail how AWS handles real-world incident \u0026amp; detection. Hands-on technical experience Learned how to analyze policies using IAM Simulator. Witnessed the actual flow of S3 public exposure → auto-remediation. Observed IR automation handling compromised EC2. Leveraging modern tools Clearly saw the role of security automation \u0026amp; multi-layer defense. Learned OIDC, cross-account guardrails, detection as code. Networking and thinking Realized how large enterprises organize security with multi-account. \u0026ldquo;Secure by design\u0026rdquo; mindset, not waiting until attacked to fix. Lessons learned Security = culture, not a feature. Misconfiguration is the biggest cause → IaC is the solution. Cannot operate cloud securely without IAM + Logging + IR Some event photos Overall, the event not only provided technical knowledge but also helped me deepen my understanding of modern security mindset, automate security processes, and improve incident response capabilities in cloud environments.\n"},{"uri":"https://crystaljohn.github.io/fcj-workshop/4-eventparticipated/4.3-event3/","title":"Event 3","tags":[],"description":"","content":"Summary Report: \u0026ldquo;AWS Cloud Mastery Series #2 workshop\u0026rdquo; Event Objectives Equip modern DevOps mindset and how to apply it on AWS ecosystem. Guide deployment of CI/CD, IaC, container orchestration, and monitoring according to AWS standards. Optimize development speed, release quality, and system reliability. Speakers FCJ Team Kha Van - Cloud Security Engineer AWS Developer Advocate Team Key Highlights AWS DevOps Services – CI/CD Pipeline Build: optimize buildspec.yml (build caching \u0026amp; parallel test). Demo: end-to-end pipeline — Commit → Build → Test → Deploy → Monitor. Deployment: must be 100% automated — no click-ops (no manual deployments). Infrastructure as Code Tools: CloudFormation / Terraform / CDK — used to deploy infrastructure (CloudFormation is AWS managed service). Template anatomy (YAML): Resources → Parameters → Outputs → Mappings. CDK model: Constructs → Stacks → Apps. Supports multiple languages (TypeScript, Python, Java\u0026hellip;). Benefits: easy to reuse, modularize, and test. When to choose: CloudFormation: declarative, suitable for enterprise teams. CDK: developer-friendly, suitable for Agile teams. \u0026ldquo;No IaC = No DevOps.\u0026rdquo;\nContainer Services on AWS Docker fundamentals Dockerfile → Build → Image → Registry → Run container Registry: Docker Hub or Amazon ECR Amazon ECR Image scanning, lifecycle policies, permissions per repository Amazon ECS Run containers with Fargate or EC2 Application Load Balancer, auto-scaling by CPU, memory, queue length Amazon EKS Managed Kubernetes service Use cases: large-scale, multi-team, portable workloads AWS App Runner For teams who want to \u0026ldquo;deploy containers like deploying to Vercel\u0026rdquo; Case study: Compare ECS, EKS, App Runner for microservices. \u0026ldquo;Containers = scalability + portability. ECS/EKS help production run smoothly.\u0026rdquo;\nMonitoring \u0026amp; Observability CloudWatch\nMetrics, Logs, Dashboards Composite alarms Custom metrics for business KPIs AWS X-Ray\nDistributed tracing Debug latency, bottlenecks, service maps Best practices\nDesign alerting to avoid noise Clear on-call workflow and runbooks Monitor using Golden Signals: Latency, Traffic, Errors, Saturation \u0026ldquo;No observability = not knowing how your system is dying.\u0026rdquo;\nKey Takeaways Modern DevOps Mindset Focus on outcome, not tools. DORA metrics: standard for measuring software performance. Continuous Integration → Continuous Delivery → Continuous Learning. Critical Technical Architecture CI/CD (AWS): CodePipeline, CodeBuild, CodeDeploy — design pipelines according to standards. IaC is the backbone of automation (CloudFormation / Terraform / CDK). Containerization helps microservices scale and be portable. Observability: detect errors quickly, reduce MTTR with metrics, logs, and tracing. DevOps Strategy Start from small pipeline → expand gradually. Use blue/green and canary to reduce deployment risk. Apply IaC + GitOps for multi-team environments. Applying to Work Build CI/CD for backend/web projects (CodePipeline / CodeBuild / CodeDeploy). Package services with Docker and deploy to ECS (Fargate) or App Runner. Use CDK to build AWS infrastructure instead of manual console operations. Monitor systems with CloudWatch (dashboards, alarms, custom metrics). Set up incident workflow for projects: alert → investigate → fix → postmortem (runbooks \u0026amp; on-call). Event Experience Attending the \u0026ldquo;AWS Cloud Mastery Series #2\u0026rdquo; workshop was an extremely valuable experience, helping me develop the mindset of a DevOps engineer. Learning from highly skilled speakers Understood in detail how AWS handles incidents \u0026amp; detection in practice. Hands-on technical experience Demo of pipeline from commit → live deployment. Demo of drift detection, CDK synth/deploy. Real-time container deployment on ECS/ECR. Leveraging modern tools CloudFormation + CDK make IaC clear and repeatable. ECR scanning enhances security. X-Ray makes debugging microservices easy. Networking and thinking Understood clearly how Dev/Product/DevOps teams collaborate in CI/CD pipelines. \u0026ldquo;Automate everything\u0026rdquo; mindset became more ingrained. Lessons learned No CI/CD → No DevOps. IaC is a prerequisite. Containers → scalability + speed. Observability → reliability. Some event photos Overall, the event not only provided technical knowledge but also helped me change my DevOps thinking, automate software development processes, and improve team collaboration.\n"},{"uri":"https://crystaljohn.github.io/fcj-workshop/4-eventparticipated/4.4-event4/","title":"Event 4","tags":[],"description":"","content":"Summary Report: \u0026ldquo;AWS Cloud Mastery Series #1 workshop\u0026rdquo; Event Objectives Introduce the AI/ML/GenAI ecosystem on AWS. Guide end-to-end ML deployment using SageMaker. Build Generative AI applications through Amazon Bedrock, RAG, Agents. Get familiar with MLOps, IaC, CICD, container workflows for AI/ML. Develop operational thinking for AI in enterprise environments. Speakers AWS AI/ML Specialist Team – AWS Vietnam AWS Solutions Architect AI/ML Community Leader in Vietnam Key Highlights AWS AI/ML Services Overview Amazon SageMaker — end-to-end ML platform\nData preparation: SageMaker Data Wrangler, Ground Truth (data labeling), Feature Store (reusable feature management) Training \u0026amp; Tuning\nTraining jobs → autoscaling GPU/CPU Supports distributed training Hyperparameter tuning (Auto-tuning, Bayesian Optimization) Deployment Options\nReal-time endpoint Serverless inference Multi-model endpoint Asynchronous (async) inference MLOps\nModel registry CI/CD pipelines using SageMaker Pipelines Monitoring drift: data drift, feature drift, model drift Live demo\nExperience SageMaker Studio: notebook, data processing, running training and deploying models. Generative AI with Amazon Bedrock Foundation models introduced\nClaude 3.5 — strong reasoning, long context, good for coding tasks Llama 3 — open-weight, suitable for fine-tuning, cost-effective Titan — high-quality embeddings for RAG (retrieval-augmented generation) Mistral — fast, lightweight Prompt engineering techniques\nZero-shot, few-shot Chain-of-Thought Role prompting Multi-step reasoning Prompt templates Best practices\nChunking inputs appropriately Metadata filtering Re-ranking Apply guardrails to ensure safety and control Bedrock Agents Agents can autonomously execute multi-step workflows. Trigger Lambda to call APIs, databases, and external workflows. Can replace backend logic in many use cases (business logic orchestration). Integrate closely with EventBridge and Step Functions in AI pipelines for orchestration and retry. CICD Workflow for Containers (ECR + ECS) Developer commit → CodeCommit CodeBuild builds the image Push image to ECR ECS pulls image to deploy (Fargate / EC2) CloudWatch monitors logs \u0026amp; metrics CodePipeline orchestrates the entire process Add security (DevSecOps):\nCodeBuild validation (unit tests, static analysis) Image scanning in ECR (vulnerability scan) Deployment policies / IAM controls to manage deployment → This is AWS-standard DevSecOps.\nKey Takeaways AI/ML and GenAI Fundamentals ML lifecycle Feature engineering Model deployment \u0026amp; monitoring Cost optimization for training/inference How to choose the right Foundation Model for use cases Advanced prompt engineering RAG architecture for production (retrieval-augmented generation) Build Agents to execute workflows automatically Critical Technical Architecture IaC (Infrastructure as Code): Terraform vs CloudFormation vs CDK Container inference workflow: ECS / ECR for AI inference containers Monitoring \u0026amp; Observability: monitoring AI workloads with CloudWatch + X-Ray Applying to Work Build enterprise chatbot with Bedrock + RAG (retrieval-augmented generation) Design end-to-end AI architecture (data → ML → app) Use Lambda + Step Functions to orchestrate RAG pipelines Use Terraform / CDK to build GenAI infrastructure Deploy inference containers via ECS / Fargate Event Experience Attending the \u0026ldquo;AWS Cloud Mastery Series #1\u0026rdquo; workshop, I clearly saw how AWS deploys real-world AI/ML, understood the differences between traditional ML and GenAI, and enhanced my AI architecture design thinking. Learning from highly skilled speakers Understood in detail how AWS builds a comprehensive AI/ML/GenAI ecosystem. Leveraging modern tools SageMaker — SageMaker helps with end-to-end ML: data preparation, training, tuning, deployment, and MLOps. Bedrock — provides Foundation Models (Claude, Llama, Titan) and supports RAG + Agents to quickly build chatbots and AI workflows. IaC \u0026amp; DevOps — CloudFormation, CDK, Terraform, CodePipeline enable controlled and automated AI/ML deployment. Data \u0026amp; ETL Tools — Glue, S3 (Data Lake), EventBridge, Step Functions support data pipelines for AI. Monitoring — CloudWatch for AI/ML monitoring, logging, and drift detection. Networking and thinking Learned to think about AI systems instead of just isolated ML models. Grasped AI/ML trends in Vietnam and how enterprises apply them. Lessons learned GenAI and traditional ML combine to create powerful AI solutions in practice. Bedrock is an enterprise GenAI platform: secure, fast to deploy, no need to train models yourself. MLOps + IaC are essential for stable AI in production. RAG is the most effective way to \u0026ldquo;bring enterprise knowledge into models.\u0026rdquo; Some event photos Overall, the event not only provided technical knowledge but also gave me a comprehensive AI/ML system design mindset, from data to deployment and operations, making me more confident in building practical AI solutions for enterprises.\n"},{"uri":"https://crystaljohn.github.io/fcj-workshop/5-workshop/1-introduction/","title":"Introduction","tags":[],"description":"","content":"Problem Statement Traditional chatbot systems struggle without the ability to access specific information from internal documents, leading to inaccurate or irrelevant responses. This workshop solves this problem by building an architecture capable of:\nAutomation: Automatically process and index PDF documents Content Inquiry: Receive queries and guide users to relevant content Document Retrieval: Answer complex questions with accurate source citations from documents Solution Architecture The system is designed following the RAG (Retrieval-Augmented Generation) model combined with AWS Serverless to ensure scalability:\nFrontend Interface: Users interact via React Web Application\nAmazon API Gateway receives requests from Frontend AWS Amplify hosting with CloudFront CDN Amazon Cognito handles authentication Request Handling:\nApplication Load Balancer routes traffic to EC2 FastAPI Backend processes REST API requests Amazon SQS (FIFO) ensures document processing order Backend Processing:\nChatHandler: Manages conversations, saves sessions to Amazon DynamoDB RAG Service: Orchestrates vector search and LLM generation Qdrant Vector Database: Self-hosted on EC2 for vector search AI \u0026amp; Data Layer:\nAmazon Bedrock: Uses Claude 3.5 Sonnet (LLM) and Cohere Embed Multilingual v3 (Embeddings) Amazon Textract: OCR and text extraction from PDF Amazon S3: Document storage Amazon DynamoDB: Metadata and chat history Admin Dashboard:\nReact-based interface hosted on AWS Amplify Upload and manage documents Monitor processing status View chat history Architecture Key Technologies In this workshop, you will work with the following key AWS services:\nAmazon Bedrock: The heart of AI, providing Foundation Models (Claude, Cohere) for language processing and embedding generation Amazon Textract: Build IDP pipeline to extract text from PDF documents Amazon EC2 \u0026amp; VPC: Compute and network infrastructure for backend services Amazon S3: Document and static asset storage Amazon DynamoDB: Store metadata, chat history, and document status Amazon Cognito: Authentication and user management AWS Amplify: Frontend application hosting with integrated CI/CD Amazon SQS: Message queue for document processing pipeline Qdrant (Self-hosted): Vector database for semantic search Terraform (IaC): Deploy entire infrastructure as code "},{"uri":"https://crystaljohn.github.io/fcj-workshop/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/08/2025 08/08/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 09/08/2025 09/08/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 10/08/2025 10/08/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 11/08/2025 11/08/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 12/08/2025 12/08/2025 https://cloudjourney.awsstudygroup.com/ Week 1 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions Set default region View EC2 service Create and manage key pairs Check information about running services Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n"},{"uri":"https://crystaljohn.github.io/fcj-workshop/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives: Understand AWS VPC and Security Get familiar with Amazon EC2 Practice all labs related to VPC Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn about Virtual Private Cloud, Subnet, Elastic Network Interface, Elastic IP Address, Internet Gateway, Nat Gateway\n- Practice AWS Identity and Access Management (IAM) 15/09/2025 15/09/2025 https://cloudjourney.awsstudygroup.com/ 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 16/09/2025 16/09/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 17/09/2025 17/09/2025 https://cloudjourney.awsstudygroup.com/ 5 - Attend AWS Cloud Day Vietnam 2025 18/09/2025 18/09/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch EC2 instance + CloudWatch Monitoring \u0026amp; Alerting 19/09/2025 19/09/2025 https://cloudjourney.awsstudygroup.com/ Week 2 Achievements: Designed and deployed VPC according to AWS Well-Architected Framework standards Subnet Route Table Public \u0026amp; Private Subnet Availability Zone (AZ) "},{"uri":"https://crystaljohn.github.io/fcj-workshop/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: Deploy and configure EC2 instances in VPC Public/Private Subnets Practice basic EC2 labs such as Site-to-Site VPN, CloudWatch Monitoring \u0026amp; Alerting Deploy Node.js application on Amazon Linux Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Practice + Deploy Production-Ready EC2 Infrastructure + How to upload aws-keypair.pem file to EC2 22/09/2025 23/09/2025 https://cloudjourney.awsstudygroup.com/ 3 - Practice + Deploy Production-Ready EC2 Infrastructure + How to connect to Private EC2 via bastion 23/09/2025 23/09/2025 https://cloudjourney.awsstudygroup.com/ 4 - Set up AWS Systems Manager Session Manager and CloudWatch Monitoring \u0026amp; Alerting for VPC resources 24/09/2025 24/09/2025 https://cloudjourney.awsstudygroup.com/ 5 - Practice + Configure Site to Site VPN 25/09/2025 25/09/2025 https://cloudjourney.awsstudygroup.com/ 6 - 26/09/2025 26/09/2025 https://cloudjourney.awsstudygroup.com/ Week 3 Achievements: Successfully deployed EC2 instances Practiced basic EC2 labs Connected to EC2 instances via SSH Created Elastic IP Address and assigned to EC2 instance Used NAT Gateway to access Internet from EC2 instance in Private Subnet Lessons \u0026amp; Difficulties: Learned how to distinguish between Public Subnet and Private Subnet. Difficulty: Initially, SSH via VSCode didn\u0026rsquo;t work, needed to adjust Security Group configuration.\n"},{"uri":"https://crystaljohn.github.io/fcj-workshop/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives: Deploy applications on Linux Instance Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Basic knowledge of virtual servers with Amazon Elastic Compute Cloud (EC2) + Clean resource 29/09/2025 29/09/2025 https://cloudjourney.awsstudygroup.com/ 3 - Go to office - Practice: + Basic Amazon EC2 + Create snapshot + Build AMI optional + Access when keypair is lost\n+ Install phpMyAdmin 30/09/2025 30/09/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 13/08/2025 13/08/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 14/08/2025 15/08/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch EC2 instance + Connect via SSH + Attach EBS volume 15/08/2025 15/08/2025 https://cloudjourney.awsstudygroup.com/ Week 4 Achievements: Understood the types of EC2 instance types to choose suitable for workload, helping optimize development costs and performance\nUse T-series instances for light workloads, low CPU demand. Use M-series instances for balanced workloads between CPU, memory, and network. Use R-series instances for workloads requiring lots of memory. Configure inbound rules in SG:\nSSH, port 22 for connection via PuTTY or SSH client All ICMP-IPv4 to allow ping and ICMP error messages All ICMP-IPv6 to allow ping and ICMP error messages over IPv6 HTTP, port 80 for unsecured web access HTTPS, port 443 for secured web access MySQL/Aurora, port 3306 for MySQL Database Custom TCP, port 5000 to run Node.js application Successfully created and configured VPC Linux and VPC Windows.\nCreated and received Windows keypair to secure EC2 instance connection via RDP (Remote Desktop Protocol) on port 3389\nConnect to Amazon Linux 2 using MobaXterm:\nChange EC2 Instance Type configuration: from t2.micro to t3.micro\nSuccessfully performed EC2 snapshot. Backed up data before changing configuration. -\u0026gt; including data backup + EBS volume state\nInstalled LAMP stack (Linux, Apache, MySQL, PHP) on Amazon Linux 2\nConfigured and used phpMyAdmin to manage database Installed Node.js Runtime Environment Deployed and ran AWS User Management application Link do-result lab: https://drive.google.com/drive/folders/1VMVLJbZfOHvjrRfTbkcXPtq55aznfbDZ?usp=sharing\n"},{"uri":"https://crystaljohn.github.io/fcj-workshop/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives: Translate AWS blog posts as assigned. Learn about Amazon RDS and basic AWS services. Practice creating and managing EC2 instances. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Practice - Translate 3 blog posts as assigned by AWS 10/06/2025 10/06/2025 https://cloudjourney.awsstudygroup.com/ 3 - Learn about Amazon Relational Database Service (Amazon RDS) 10/07/2025 10/07/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 10/08/2025 10/08/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 10/09/2025 10/09/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 10/10/2025 10/10/2025 https://cloudjourney.awsstudygroup.com/ Week 5 Achievements: Completed translating 3 AWS blog posts as assigned. Understood the basic concepts of Amazon RDS (Relational Database Service). Successfully created an AWS Free Tier account and configured AWS CLI. Mastered basic EC2 knowledge: Instance types, AMI, EBS, Elastic IP. Successfully practiced creating an EC2 instance, connecting via SSH, and attaching an EBS volume. "},{"uri":"https://crystaljohn.github.io/fcj-workshop/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":" ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 6 Objectives: Understand knowledge related to Route 53 in the Reliability pillar of the AWS Well-Architected Framework. Tasks to be completed this week: Day Task Start Date End Date Reference Material 2 - Get familiar with Route 53 + Definition and functions 13/10/2025 13/10/2025 3 - Practice: - Set up Hybrid DNS with Route 53 Resolver 14/10/2025 14/10/2025 https://cloudjourney.awsstudygroup.com/ 4 - Learn about concepts: + Principle of Least Privilege (grant only necessary permissions) + How KMS (Key Management Service) works 15/10/2025 15/10/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn about Amazon DynamoDB and practice lab on DynamoDB - Learn about creating Global Secondary Index 16/10/2025 16/10/2025 https://000060.awsstudygroup.com/vi/1-introduce/ 6 - Practice: + Develop with Python and DynamoDB 17/10/2025 17/10/2025 https://000060.awsstudygroup.com/vi/3-gettingstartedwithawssdk/3.2-pythonandynamodb/ Week 6 Achievements: Understood how to design DNS systems with Route 53 Resolver\nUsed AWS Quick Start to deploy Hybrid DNS Used AWS Managed Microsoft Active Directory Practice result: https://drive.google.com/drive/folders/1-ZtnhA9PyAjANC9loPAZApB7TQHBCAox?usp=sharing Learned that creating a Global Secondary Index (GSI) in DynamoDB allows for fast queries based on attributes other than the primary key (PK). Instead of using PK with Partition Key and Sort Key for queries:\nPartition Key: Attribute used to distribute data across physical storage partitions. Sort Key: Attribute used to sort and filter data within the same partition. "},{"uri":"https://crystaljohn.github.io/fcj-workshop/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Practice: Use Lambda function to optimize costs for your system on AWS environment 10/24/2025 10/24/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 10/27/2025 10/27/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 10/28/2025 10/28/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 10/29/2025 10/30/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 10/31/2025 10/31/2025 https://cloudjourney.awsstudygroup.com/ Week 8 Achievements: Learned how to use Lambda function to optimize costs for your system on AWS environment. Work log for Day 2: https://drive.google.com/file/d/1FvJtA5q1DCmf5l2AToMHMPDEDIHK1EE-/view?usp=drive_link "},{"uri":"https://crystaljohn.github.io/fcj-workshop/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"Week 9 Objectives: Complete a new AWS account to prepare for the project proposal. Deeply understand S3 and related concepts. Design architecture for the proposal project. Tasks to be completed this week: Day Task Start Date End Date Reference Material 2 - Practice: Strategy to receive $200 credit (Lab by mentor Thinh Nguyen) + Create EC2 + Amazon Bedrock Playground + Set up AWS Budgets + Create Lambda Web App + Create RDS Database 03/11/2025 03/11/2025 https://thinhnguyen1211.github.io/fcj-first-lab/ 3 - Learn about Amazon S3 - Access Point - Storage Class - Learn about AWS Security Hub 04/11/2025 04/11/2025 https://www.youtube.com/watch?v=_yunukwcAwc\u0026t=2s https://www.youtube.com/watch?v=YnLo4MgOXyA 4 - Practice: Serverless lab - Guide to writing Front-end calling API Gateway - Work with Amazon DynamoDB 05/11/2025 05/11/2025 https://000079.awsstudygroup.com/vi/ 5 - Learn about Amazon DynamoDB - Design architecture for proposal project 06/11/2025 06/11/2025 https://cloudjourney.awsstudygroup.com/ 6 - Design architecture for proposal project 07/11/2025 07/11/2025 https://cloudjourney.awsstudygroup.com/ Week 9 Achievements: AWS account credit expired, so a new account was created to continue practicing.\nPractice result: successfully received $200 credit. Created Budgets Cost to monitor AWS service usage costs. Daily Cost Monitoring Identified \u0026ldquo;dangerous\u0026rdquo; services to avoid Understood S3 Access Point and Storage Class.\nS3 Access Point helps manage data access permissions in S3 buckets more easily, especially in environments with many users or applications. Storage Class in S3 divides storage into different classes to optimize cost and performance based on data access frequency. S3 Standard: Frequently accessed data. S3 Standard-IA: Infrequently accessed data. Storing frequently accessed data in this class will incur higher costs than S3 Standard. S3 Intelligent-Tiering: Automatically moves objects between storage tiers based on days without access. S3 One Zone-IA: Infrequently accessed data, stored in a single zone only, not replicated. Amazon Glacier / Deep Archive: Long-term data storage. Data cannot be directly GET from this class; it must be restored to one of the four classes above. Understood AWS Backup, which sets up automatic data protection policies for AWS services and on-premises environments.\nUnderstood RESTful API and WebSocket API:\nRESTful API creates a new connection for each client request to the server. After the server responds, the connection is closed. WebSocket API maintains an open connection between client and server, allowing continuous two-way communication without re-establishing the connection for each request. -\u0026gt; Suitable for real-time applications like chat, games, etc. Issues encountered:\nDuring the lab, did not understand DynamoDB well enough to work with it and had to research basic concepts of DynamoDB. "},{"uri":"https://crystaljohn.github.io/fcj-workshop/3-blogstranslated/3.2-blog2/","title":"Build end-to-end Apache Spark pipelines with Amazon MWAA, Batch Processing Gateway, and Amazon EMR on EKS","tags":[],"description":"","content":"By: Avinash Desireddy and Suvojit Dasgupta Date: May 1, 2025\nTopics: Amazon EMR on EKS, Amazon Managed Workflows for Apache Airflow (Amazon MWAA), AWS Big Data, Intermediate Level (200), Open Source\nApache Spark workloads running on Amazon EMR on EKS are foundational to many modern data platforms. EMR on EKS provides benefits by offering a managed Spark environment, integrating seamlessly with other AWS services, and your organization\u0026rsquo;s existing Kubernetes-based deployment patterns.\nData platforms processing large-scale data volumes often require multiple EMR on EKS clusters. In the post Use Batch Processing Gateway to automate job management in multi-cluster Amazon EMR on EKS environments, we introduced Batch Processing Gateway (BPG) as a solution for managing Spark workloads across these clusters. Although BPG provides foundational functionality for workload distribution and routing support for Spark jobs in multi-cluster environments, enterprise data platforms demand additional features for a comprehensive data processing pipeline.\nThis post shows how to enhance the multi-cluster solution by integrating Amazon Managed Workflows for Apache Airflow (Amazon MWAA) with BPG. By using Amazon MWAA, we add job scheduling and orchestration capabilities, enabling you to build a comprehensive end-to-end Spark-based data processing pipeline.\nSolution overview Consider HealthTech Analytics, a healthcare analytics company managing two distinct data processing workloads. Their Clinical Insights Data Science team processes sensitive patient outcome data, requiring HIPAA compliance and dedicated resources. Meanwhile, the Digital Analytics team handles website interaction data with more flexible requirements. As their operations have grown, they face increasing challenges in managing these diverse workloads efficiently.\nThe company needs to maintain strict separation between processing protected health information (PHI) and non-PHI data, while addressing different cost center requirements. The Clinical Insights team runs critical end-of-day batch processes that need guaranteed resources, whereas the Digital Analytics team can use cost-optimized spot instances for their variable workloads. Additionally, data scientists from both teams require environments for experimentation and prototyping on demand.\nThis scenario is an ideal use case for implementing a data pipeline using Amazon MWAA, BPG, and multiple EMR on EKS clusters. The solution needs to route different Spark workloads to appropriate clusters based on security requirements and cost profiles, while maintaining necessary isolation and compliance controls. To efficiently manage such an environment, we need a solution that maintains clear separation between application and infrastructure management concerns, and connects multiple components together into a robust pipeline.\nOur solution involves integrating Amazon MWAA with BPG through an Airflow custom operator for BPG called the BPGOperator. This operator encapsulates the infrastructure management logic required to interact with BPG. The BPGOperator provides a clean interface for job submission through Amazon MWAA. When invoked, the operator communicates with BPG, which then routes Spark workloads to available EMR on EKS clusters based on predefined routing rules.\nThe following architecture diagram illustrates the components and their interactions.\nThe solution works through the following steps:\nAmazon MWAA runs scheduled DAGs using the BPGOperator. Data engineers create DAGs using this operator, requiring only the Spark application configuration file and basic scheduling parameters. BPGOperator validates and submits jobs to the BPG endpoint POST:/apiv2/spark. It handles all HTTP communication details, manages authentication tokens, and provides secure transmission of job configurations. BPG routes submitted jobs to EMR on EKS clusters based on predefined routing rules. These rules are centrally managed through BPG\u0026rsquo;s configuration, enabling rule-based workload distribution across multiple clusters. BPGOperator monitors job status, logs, and handles execution retries. It polls the BPG job status endpoint GET:/apiv2/spark/{subID}/status and streams logs to Airflow by polling the GET:/apiv2/log endpoint every second. The BPG log endpoint retrieves the latest log information directly from the Spark Driver Pod. DAG execution proceeds to subsequent tasks based on job completion status and defined dependencies. The BPGOperator communicates job status through Airflow\u0026rsquo;s built-in task communication system, enabling complex workflow orchestration. Refer to BPG\u0026rsquo;s REST API interface documentation for more details.\nThis architecture provides several key benefits:\nSeparation of concerns – Data Engineering and Platform Engineering teams in enterprise organizations often maintain distinct responsibilities. The modular design in this solution allows platform engineers to configure the BPGOperator and manage EMR on EKS clusters, while data engineers maintain DAGs. Centralized code management – The BPGOperator encapsulates all core functionality needed for Amazon MWAA DAGs to submit Spark jobs through BPG into a single, reusable Python module. This centralization minimizes code duplication across DAGs and improves maintainability by providing a standardized interface for job submission. Airflow custom operator for BPG An Operator in Airflow is a template for a predefined Task that you can define declaratively inside your DAG. Airflow provides many built-in operators such as BashOperator to run bash commands, PythonOperator to run Python functions, and EmrContainerOperator to submit a new job to an EMR on EKS cluster. However, there is no built-in operator that performs all the steps required for Amazon MWAA integration with BPG.\nAirflow allows you to create new operators to fit your specific requirements. This type of operator is called a custom operator. A custom operator encapsulates custom infrastructure-related logic into a single, maintainable component. Custom operators are created by extending the airflow.models.baseoperator.BaseOperator class. We developed and open-sourced an Airflow custom operator for BPG called BPGOperator, which performs the steps required to provide seamless integration of Amazon MWAA with BPG.\nThe following class diagram provides a detailed view of the BPGOperator implementation.\nWhen a DAG includes a BPGOperator task, the Amazon MWAA instance triggers the operator to submit a job request to BPG. The operator typically performs the following steps:\nJob initialization – The BPGOperator prepares the job payload, including input parameters, configuration, connection details, and other metadata that BPG requires. Job submission – The BPGOperator handles HTTP POST requests to submit jobs to BPG endpoints with the provided configurations. Job execution monitoring – The BPGOperator checks the job status, polling BPG until the job completes successfully or fails. The monitoring process includes handling various job states, managing timeout scenarios, and responding to errors that occur during job execution. Job completion handling – Upon completion, the BPGOperator logs the job results, records relevant details, and can trigger subsequent tasks based on execution results. The following sequence diagram illustrates the interaction flow between the Airflow DAG, BPGOperator, and BPG.\nDeploying the solution In the remainder of this post, you will deploy an end-to-end pipeline to run Spark jobs across multiple EMR on EKS clusters. You will start by deploying the common components that form the foundation for building pipelines. Next, you will deploy and configure BPG on an EKS cluster, followed by deploying and configuring the BPGOperator on Amazon MWAA. Finally, you will run Spark jobs across multiple EMR on EKS clusters from Amazon MWAA.\nTo streamline the installation process, we have automated the deployment of all infrastructure components required for this post, so you can focus on the essential aspects of job submission to build an end-to-end pipeline. We provide detailed information to help you understand each step, simplifying setup while maintaining the learning experience.\nTo demonstrate the solution, you will create three clusters and one Amazon MWAA environment:\nTwo EMR on EKS clusters: analytics-cluster and datascience-cluster One EKS cluster: gateway-cluster One Amazon MWAA environment: airflow-environment The analytics-cluster and datascience-cluster serve as data processing clusters running Spark workloads, gateway-cluster hosts BPG, and airflow-environment hosts Airflow for job orchestration and scheduling.\nYou can find the source code in the GitHub repository.\nPrerequisites Before deploying this solution, make sure the following prerequisites are met:\nAccess to a valid AWS account AWS Command Line Interface (AWS CLI) installed on your local machine Git, Docker, eksctl, kubectl, Helm, and jq utilities installed on your local machine Permissions to create AWS resources Familiarity with Kubernetes, Amazon MWAA, Apache Spark, Amazon Elastic Kubernetes Service (Amazon EKS), and Amazon EMR on EKS Common infrastructure setup This step handles network infrastructure setup, including virtual private cloud (VPC) and subnets, along with configuring AWS Identity and Access Management (IAM) roles, Amazon Simple Storage Service (Amazon S3) storage, Amazon Elastic Container Registry (Amazon ECR) repositories for BPG images, Amazon Aurora PostgreSQL-Compatible Edition database, Amazon MWAA environment, and both EKS and EMR on EKS clusters with a preconfigured Spark operator. With this infrastructure provisioned automatically, you can focus on the subsequent steps without being bogged down by basic setup tasks.\nClone the repository to your local machine and set two environment variables. Replace \u0026lt;AWS_REGION\u0026gt; with the AWS Region where you want to deploy these resources.\ngit clone https://github.com/aws-samples/sample-mwaa-bpg-emr-on-eks-spark-pipeline.git cd sample-mwaa-bpg-emr-on-eks-spark-pipeline export REPO_DIR=$(pwd) export AWS_REGION=\u0026lt;AWS_REGION\u0026gt; Run the following script to create the common infrastructure:\ncd ${REPO_DIR}/infra ./setup.sh To verify successful infrastructure deployment, navigate to the AWS CloudFormation console, open your stack, and check the Events, Resources, and Outputs tabs to see the completion status, details, and list of created resources.\nYou have completed setting up the common components that form the foundation for the rest of the deployment.\nBatch Processing Gateway setup This section builds the Docker image for BPG, deploys the helm chart on the EKS cluster gateway-cluster, and exposes the BPG endpoint using a Kubernetes service of type LoadBalancer. Complete the following steps:\nDeploy BPG on the EKS cluster gateway-cluster: cd ${REPO_DIR}/infra/bpg ./configure_bpg.sh Verify the deployment by listing pods and viewing the pod logs: kubectl get pods --namespace bpg kubectl logs \u0026lt;BPG-PODNAME\u0026gt; --namespace bpg Review the logs and confirm there are no errors or exceptions.\nExec into the BPG pod and verify the health check: kubectl exec -it \u0026lt;BPG-PODNAME\u0026gt; -n bpg -- bash curl -u admin:admin localhost:8080/skatev2/healthcheck/status The healthcheck API will return a successful response of {\u0026quot;status\u0026quot;:\u0026quot;OK\u0026quot;}, confirming successful BPG deployment on the EKS cluster gateway-cluster.\nWe have successfully configured BPG on the gateway-cluster and set up EMR on EKS for both the datascience-cluster and analytics-cluster. This is the point where we stopped in the previous blog. In the following steps, we will configure Amazon MWAA with the BPGOperator, then write and submit DAGs to illustrate an end-to-end Spark-based data pipeline.\nConfiguring the Airflow operator for BPG on Amazon MWAA This section configures the BPGOperator plugin on the Amazon MWAA environment airflow-environment.\nConfigure the BPGOperator on Amazon MWAA: cd ${REPO_DIR}/bpg_operator ./configure_bpg_operator.sh On the Amazon MWAA console, navigate to the airflow-environment environment. Choose Open Airflow UI, and in the Airflow interface, choose the Admin dropdown menu and choose Plugins. You will see the BPGOperator plugin listed in the Airflow interface. Configuring Airflow connections for BPG integration This section walks you through setting up Airflow connections that enable secure communication between the Amazon MWAA environment and BPG. The BPGOperator uses the configured connection to authenticate and interact with BPG endpoints.\nRun the following script to configure the Airflow connection bpg_connection:\ncd $REPO_DIR/airflow ./configure_connections.sh In the Airflow interface, choose the Admin dropdown menu and choose Connections. You will see bpg_connection listed.\nConfiguring Airflow DAG to run Spark jobs This step configures an Airflow DAG to run a sample application. Specifically, we will submit a DAG containing multiple sample Spark jobs using Amazon MWAA to EMR on EKS clusters using BPG. Please wait a few minutes for the DAG to appear in the Airflow interface.\ncd $REPO_DIR/jobs ./configure_job.sh Triggering the Amazon MWAA DAG In this step, we trigger the Airflow DAG and observe the job execution behavior, including reviewing the Spark logs in the Airflow interface:\nIn the Airflow interface, review the DAG MWAASparkPipelineDemoJob and choose the play icon to trigger the DAG. Wait for the DAG to complete successfully. When the DAG completes successfully, you will see Success:1 under the Runs column. In the Airflow interface, find and choose the DAG MWAASparkPipelineDemoJob. On the Graph tab, choose any task (for example, we choose the task calculate_pi) and then choose Logs. View the Spark logs in the Airflow interface. Migrating existing Airflow DAGs to use BPG In enterprise data platforms, a typical data pipeline consists of Amazon MWAA submitting Spark jobs to multiple EMR on EKS clusters using the SparkKubernetesOperator and an Airflow Connection of type Kubernetes. An Airflow Connection is a set of parameters and credentials used to establish communication between Amazon MWAA and external systems or services. A DAG references the connection name and connects to the external system.\nThe following diagram shows the typical architecture.\nIn this setup, Airflow DAGs typically use SparkKubernetesOperator and SparkKubernetesSensor to submit Spark jobs to a remote EMR on EKS cluster using kubernetes_conn_id=\u0026lt;connection_name\u0026gt;.\n# Submit Spark-Pi job using Kubernetes connection submit_spark_pi = SparkKubernetesOperator( task_id=\u0026#39;submit_spark_pi\u0026#39;, namespace=\u0026#39;default\u0026#39;, application_file=spark_pi_yaml, kubernetes_conn_id=\u0026#39;emr_on_eks_connection_[1|2]\u0026#39;, # Connection ID defined in Airflow dag=dag) To migrate the infrastructure to BPG-based infrastructure without affecting environment continuity, we can deploy a parallel infrastructure using BPG, create a new Airflow Connection for BPG, and gradually migrate DAGs to use the new connection. By doing so, we will not disrupt the existing infrastructure until the BPG-based infrastructure is fully operational, including migrating all existing DAGs.\nThe following diagram introduces the transient state where both Kubernetes connections and BPG connections are operational. Blue arrows indicate existing workflow paths, and red arrows represent new BPG-based migration paths.\nThe modified code snippet for the DAG is as follows:\n# Submit Spark-Pi job using BPG connection submit_spark_pi = BPGOperator( task_id=\u0026#39;submit_spark_pi\u0026#39;, application_file=spark_pi_yaml, application_file_type=\u0026#39;yaml\u0026#39;, connection_id=\u0026#39;bpg_connection\u0026#39;, # Connection ID defined in Airflow dag=dag) Finally, when all DAGs have been modified to use BPGOperator instead of SparkKubernetesOperator, you can decommission any remnants of the old workflow. The final state of the infrastructure will look like the following diagram.\nUsing this approach, we can seamlessly integrate BPG into an environment that currently only uses Amazon MWAA and EMR on EKS clusters.\nCleanup To avoid incurring future charges from resources created in this walkthrough, clean up your environment after you have completed the steps. You can do this by running the cleanup.sh script, which will safely delete all resources provisioned during setup:\ncd ${REPO_DIR}/setup ./cleanup.sh Conclusion In the post Use Batch Processing Gateway to automate job management in multi-cluster Amazon EMR on EKS environments, we introduced Batch Processing Gateway as a solution for routing Spark workloads across multiple EMR on EKS clusters. In this post, we demonstrated how to enhance this foundation by integrating BPG with Amazon MWAA. Through our custom BPGOperator, we showed how to build robust end-to-end Spark-based data processing pipelines while maintaining clear separation of concerns and centralized code management. Finally, we demonstrated how to seamlessly integrate the solution into your existing Amazon MWAA and EMR on EKS data platform without affecting operational continuity.\nWe encourage you to experiment with this architecture in your own environment, adapting it to fit your unique workloads and operational requirements. By deploying this solution, you can build efficient and scalable data processing pipelines, harnessing the full potential of EMR on EKS and Amazon MWAA. Explore further by deploying the solution in your AWS account while adhering to your organization\u0026rsquo;s security best practices and share your experiences with the AWS Big Data community.\nAbout the authors Suvojit Dasgupta is a Principal Data Architect at AWS. He leads a team of skilled engineers in designing and building scalable data solutions for AWS customers. He specializes in developing and deploying innovative data architectures to solve complex business challenges.\nAvinash Desireddy is a Cloud Infrastructure Architect at AWS, passionate about building secure data applications and platforms. He has deep experience in Kubernetes, DevOps, and enterprise architecture, helping customers containerize applications, streamline deployments, and optimize cloud-native environments.\n"},{"uri":"https://crystaljohn.github.io/fcj-workshop/5-workshop/2-preparation/","title":"Preparation","tags":[],"description":"","content":"Prerequisites Requirements needed to complete this workshop:\nAWS Client Machine: Configured with access to necessary AWS services Development Environment: Windows, macOS, or Linux with basic development tools Basic Knowledge: Understanding of AWS, Python, JavaScript, and Docker GitHub Account: To clone source code and track changes AWS Budget: Approximately $65/month for resources (EC2, Bedrock, NAT Gateway) Tool Installation 1. AWS CLI AWS Command Line Interface (AWS CLI) is a tool to interact with AWS services.\nWindows:\n# Download and install MSI installer msiexec.exe /i https://awscli.amazonaws.com/AWSCLIV2.msi macOS:\nbrew install awscli Linux:\ncurl \u0026#34;https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\u0026#34; -o \u0026#34;awscliv2.zip\u0026#34; unzip awscliv2.zip sudo ./aws/install Verify installation:\naws --version # aws-cli/2.x.x Python/3.x.x 2. Terraform Terraform is an Infrastructure as Code tool to provision AWS resources.\nWindows:\nchoco install terraform macOS:\nbrew tap hashicorp/tap brew install hashicorp/tap/terraform Linux:\nwget -O- https://apt.releases.hashicorp.com/gpg | sudo gpg --dearmor -o /usr/share/keyrings/hashicorp-archive-keyring.gpg echo \u0026#34;deb [signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $(lsb_release -cs) main\u0026#34; | sudo tee /etc/apt/sources.list.d/hashicorp.list sudo apt update \u0026amp;\u0026amp; sudo apt install terraform Verify:\nterraform --version 3. Docker Docker to run Qdrant vector database locally and on EC2.\nWindows/macOS: Download Docker Desktop\nLinux:\ncurl -fsSL https://get.docker.com -o get-docker.sh sudo sh get-docker.sh sudo usermod -aG docker $USER Verify:\ndocker --version docker run hello-world 4. Node.js (\u0026gt;= 18) Node.js for frontend development with React + Vite.\nUsing nvm (recommended):\n# Install nvm curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.0/install.sh | bash # Install Node.js 18 nvm install 18 nvm use 18 Verify:\nnode --version # v18.x.x npm --version # 9.x.x 5. Python (\u0026gt;= 3.11) Python for backend FastAPI application.\nWindows: Download from python.org (select \u0026ldquo;Add to PATH\u0026rdquo;)\nmacOS:\nbrew install python@3.11 Linux:\nsudo apt update sudo apt install python3.11 python3.11-venv python3-pip Verify:\npython --version # Python 3.11.x pip --version 6. Git Git for version control.\nWindows: Download from git-scm.com\nmacOS:\nbrew install git Linux:\nsudo apt install git Verify:\ngit --version Clone Repository git clone https://github.com/CrystalJohn/ARC-project.git cd ARC-project Configure AWS Credentials Create IAM User Login to AWS Console Navigate to IAM → Users → Create user User name: arc-workshop-user Attach policies: AmazonEC2FullAccess AmazonS3FullAccess AmazonDynamoDBFullAccess AmazonCognitoPowerUser AmazonSQSFullAccess AmazonTextractFullAccess AmazonBedrockFullAccess CloudWatchFullAccess IAMFullAccess Create access key → Download credentials Configure AWS CLI aws configure Enter information:\nAWS Access Key ID: AKIAXXXXXXXXXXXXXXXX AWS Secret Access Key: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx Default region name: ap-southeast-1 Default output format: json Verify:\naws sts get-caller-identity Activate Amazon Bedrock Models Request Model Access AWS Console → Amazon Bedrock → Model access Click Manage model access Select models: Anthropic - Claude 3.5 Sonnet (anthropic.claude-3-5-sonnet-20241022-v2:0) Cohere - Embed Multilingual v3 (cohere.embed-multilingual-v3) Click Request model access → Accept Terms → Submit Verify Access # Test Claude aws bedrock get-foundation-model \\ --model-identifier anthropic.claude-3-5-sonnet-20241022-v2:0 \\ --region ap-southeast-1 # Test Cohere aws bedrock get-foundation-model \\ --model-identifier cohere.embed-multilingual-v3 \\ --region ap-southeast-1 Expected: Status Access granted\nPrepare Sample Documents Project comes with sample PDFs in samples/:\nls samples/ # data-structures-sample.pdf # test-sample.pdf Document Requirements Limit Value Format PDF (text-based or scanned) Max size 50 MB Max pages 500 pages Recommended 10-100 pages Checklist Before proceeding, ensure:\nAWS CLI installed and configured Terraform installed Docker installed and running Node.js 18+ installed Python 3.11+ installed Git installed Repository cloned IAM user created with proper permissions Bedrock models approved (Claude + Cohere) Sample documents ready "},{"uri":"https://crystaljohn.github.io/fcj-workshop/2-proposal/","title":"Proposal","tags":[],"description":"","content":"Academic Research Chatbot AWS RAG-based solution for smart academic support and research 1. Executive Summary Academic Research Chatbot is an AI assistant supporting academic research, helping students and lecturers search, summarize, and analyze scientific documents (PDFs, papers) through natural conversation with accurate source citations.\nKey Highlights:\nCore Technology: Combines IDP (Amazon Textract) to process documents (including scans) and RAG (Amazon Bedrock - Claude 3.5 Sonnet) to generate intelligent responses. Optimized Architecture: Hybrid model using 1 EC2 t3.small combined with Serverless services (Amplify, Cognito, S3, DynamoDB) to balance performance and cost. Feasibility: Serves ~50 internal users with operating costs ~$60/month, fast deployment time (20 days), and maximizes AWS Free Tier. 2. Problem Statement Current Problem Students and researchers have to work with a large number of academic documents (conference papers, journals, theses, technical reports). Many documents are old scanned PDFs (pre-2000), without a text layer, making searching for content, data, and tables very time-consuming. Public AI tools (ChatGPT, Perplexity, NotebookLM, etc.) are not directly connected to the school/department\u0026rsquo;s internal document repository, making it difficult to ensure security and access rights by subject or research group. The current infrastructure lacks a unified access point to:\nManage research documents by subject/topic. Allow researchers to ask questions directly on their own papers. Ensure answers have clear citations (paper, page, table, section). Consequence: Researchers have to manually read, take notes, and copy data from multiple papers; lecturers find it difficult to quickly synthesize information when preparing lectures or topics; academic data is scattered across many personal machines, difficult to standardize and reuse. Solution Academic Research Chatbot proposes building an internal academic Q\u0026amp;A platform based on AWS, where:\nDev/Admin loads research document repository: Upload PDFs to Amazon S3, metadata is stored in Amazon DynamoDB. An EC2 worker consumes the Amazon SQS queue, calls Amazon Textract to OCR, extract text, tables, forms, including scanned documents. Worker normalizes/chunks content, sends to Amazon Bedrock Titan Text Embeddings v2 to generate embeddings, and indexes into Qdrant on EC2. Researchers ask questions via web interface (Amplify + CloudFront): Questions are embedded, querying Qdrant to retrieve the most relevant segments (Retrieval). These segments are passed to Claude 3.5 Sonnet on Amazon Bedrock to generate answers with accurate citations (paper, page, section, table) and explanations in academic context. All access is protected by Amazon Cognito (researcher vs admin authorization), logs \u0026amp; metrics are monitored via Amazon CloudWatch + SNS (alerts on worker errors, queue backlog, high EC2 CPU). Benefits and ROI Academic Efficiency:\nReduces 40–60% of time researchers spend finding data, F1-scores, p-values, sample sizes, experimental equipment, or method descriptions from multiple papers. Reduces citation errors due to forgetting pages/tables, as the chatbot always returns sources and locations. Internal Knowledge Management: Research documents are centralized in an S3 + DynamoDB repository, easy to backup, authorize, and expand. Can be reused for many courses, topics, and labs without building a new system. Low \u0026amp; Controllable Infrastructure Costs: Hybrid model 1 EC2 + managed AI services keeps operating costs for 50 internal users at around \u0026lt; $50/month, mainly paying for EC2, 2–3 VPC endpoint interfaces, and Bedrock/Textract usage. The system is designed to be deployed in about 20 days by a team of 4, suitable for a research/internship project but still has product architecture quality. Long-term Value: Creates a platform to later integrate learning behavior analysis dashboards, paper recommendation modules, or expand to multi-language and multi-field learning assistants. 3. Solution Architecture Academic Research Chatbot applies the AWS Hybrid RAG Architecture model with IDP (Intelligent Document Processing), combining a single EC2 (FastAPI + Qdrant + Worker) with managed AI services (Textract, Bedrock) to optimize costs while ensuring performance for about 50 internal users.\nData Processing and Conversation Flow AWS Services Used\nAmazon Route 53: DNS management for chatbot platform domain. Amazon CloudFront: CDN distributing web interface (chat + admin) with low latency. AWS Amplify Hosting: Hosts web application (React/Next) for Researchers and Dev/Admin. Amazon Cognito: User authentication, researcher vs admin role management. Amazon S3: Stores original PDF files uploaded by Dev/Admin (raw documents). Amazon SQS (doc_ingestion_queue): Job queue for document processing. Amazon Textract: IDP/OCR for scanned PDFs and digital PDFs. Amazon Bedrock: Titan Text Embeddings v2: Generates embedding vectors for text chunks. Claude 3.5 Sonnet: Generates academic answers from context + user questions (RAG). Amazon DynamoDB: Documents table: document metadata, pipeline status (UPLOADED, IDP_RUNNING, EMBEDDING_DONE, FAILED). Amazon EC2 (t3.small, private subnet): Runs FastAPI backend (REST API for chat and admin). Runs Qdrant Vector DB to store and query embeddings. Runs Worker process consuming SQS, calling Textract + Titan, indexing into Qdrant, updating DynamoDB. VPC + ALB + VPC Endpoints: VPC + private subnet for EC2 (not directly exposed to Internet). Application Load Balancer (ALB): entry point for all APIs from Amplify to EC2. Gateway Endpoint (S3, DynamoDB) and Interface Endpoint (Textract, Bedrock, SQS – if used) for EC2 to call AWS services without NAT Gateway. Amazon CloudWatch + Amazon SNS: Collects logs and metrics from EC2, ALB, SQS. CloudWatch Alarms send alerts via SNS when CPU is high, SQS backlog, worker errors, etc. AWS CodePipeline / CodeBuild: Automates build \u0026amp; deploy for backend (FastAPI on EC2). Component Design\nUsers: Researchers: Q\u0026amp;A, academic content lookup. Dev/Admin: Upload, manage, and re-index documents. Document Processing (IDP): PDFs uploaded by Dev/Admin to S3. Worker on EC2 calls Textract for OCR and text/table extraction. Indexing \u0026amp; Vector DB: Worker normalizes, chunks content. Calls Bedrock Titan Embeddings v2 to create embeddings. Saves embeddings + metadata to Qdrant on EC2. AI Conversation (RAG): FastAPI embeds question, queries Qdrant for top-k relevant segments. Sends context + question to Claude 3.5 Sonnet (Bedrock) to generate answer with citation. User Management: Cognito authenticates and authorizes researcher / admin. Storage \u0026amp; State: DynamoDB stores document metadata (doc_id, status, owner, …) and (optional) chat history. 4. Technical Implementation Implementation Phases The project consists of 2 main parts — web platform (UI + auth) and RAG + IDP backend — deployed across 4 phases:\nResearch \u0026amp; Architecture Finalization: Review requirements (50 researchers, 1 EC2, IDP + RAG). Finalize architecture: VPC, EC2 (FastAPI + Qdrant + Worker), Amplify, Cognito, S3, SQS, DynamoDB, Textract, Bedrock. POC \u0026amp; Connectivity Check: Create EC2, VPC endpoints, test calling Textract, Titan Embeddings, Claude 3.5 Sonnet. Run simple Qdrant on EC2, test vector insert/search. Create skeleton FastAPI + a minimal Chat UI on Amplify. Feature Completion: Build /api/chat (FastAPI) + RAG pipeline: embed query → Qdrant → Claude + citation. Build /api/admin/: upload PDF, save to S3 + DynamoDB, push message to SQS. Write Worker on EC2: SQS → Textract → normalize/chunk → Titan → Qdrant → update DynamoDB. Complete Chat UI and Admin UI (upload + view document status). Testing, Optimization, Internal Demo Deployment: End-to-end test with a set of ~50–100 papers. Add CloudWatch Logs/Alarms, SNS notify on error or queue backlog. Adjust EC2, Qdrant configuration, batch size to optimize time and cost. Prepare user guide and demo for the group of 50 researchers. Technical Requirements\nFrontend \u0026amp; Auth: React/Next.js hosted on AWS Amplify, CloudFront CDN, Route 53 DNS. Amazon Cognito manages identity and permissions (Researcher/Admin). Backend \u0026amp; Compute: EC2 t3.small (Private Subnet) running All-in-one: FastAPI, Qdrant Vector DB, and Worker. Asynchronous processing: Worker reads SQS, triggers Textract and Bedrock to index data. IDP \u0026amp; RAG: Storage: S3 (Original files), DynamoDB (Metadata \u0026amp; Status). AI Core: Textract (OCR scanned docs), Bedrock Titan (Embedding), Claude 3.5 Sonnet (Question Answering). Network \u0026amp; Observability: Network: VPC Private Subnet, VPC Endpoints for secure connection to AWS Services. Monitoring: CloudWatch Logs/Metrics + SNS alerts (High CPU, Worker errors). 5. Timeline \u0026amp; Milestones The project is executed over approximately 6 weeks with specific phases:\nWeek 1-2 (Days 1-10): Research \u0026amp; Design Detailed architecture design, scope definition, service selection. Planning for operational cost optimization and deployment. Week 3 (Days 11-15): AWS Infrastructure Setup Configure VPC, Subnets, Security Groups, IAM Roles. Deploy EC2 t3.small, S3 bucket, DynamoDB tables. Setup VPC Endpoints (Gateway + Interface). Week 4 (Days 16-20): Backend APIs \u0026amp; IDP Pipeline Build FastAPI endpoints (/api/chat, /api/admin/upload). Integrate IDP pipeline: SQS → Worker → Textract → Embeddings → Qdrant. Connect Bedrock (Titan Embeddings + Claude 3.5 Sonnet). Week 5 (Days 21-25): Testing \u0026amp; Error Handling End-to-end testing with a set of ~50-100 papers. Handle edge cases, retry logic, error handling. Optimize chunking strategy and retrieval accuracy. Week 6 (Days 26-30): Deployment \u0026amp; Documentation Finalize UI/UX for Admin and Researcher. Setup CloudWatch Alarms + SNS notifications. Prepare user guide and demo for the group of 50 researchers. 6. Budget Estimation You can view costs on the AWS Pricing Calculator Or download the Budget Estimation File.\nInfrastructure Costs (Estimated Monthly)\nFixed Infrastructure (~$40–45): Compute \u0026amp; Network: EC2 t3.small ($15) + VPC Endpoints ($20). Storage \u0026amp; Web: S3, DynamoDB, SQS, Amplify, CloudWatch (~$5–10). AI Costs (Variable): Amazon Textract: ~$15–25 (batch processing first 10,000 pages). Amazon Bedrock: $5–15 (serving 50 users). Total: **$50–60/month** for internal research environment. 7. Risk Assessment Risk Matrix\nHallucination (AI fabrication): High impact, medium probability. Budget Overrun (AI Services): Medium impact, medium probability. Infrastructure Failure (EC2/Qdrant): High impact, low probability. Mitigation Strategies\nAI Quality: Mandatory source citations, limit input context from Qdrant. Cost: Set up AWS Budgets/Alarms, control document ingestion volume. Infrastructure \u0026amp; Security: Periodic EBS backups, data encryption (S3/DynamoDB), strict permissions via Cognito/IAM. Contingency Plans\nSystem Failure: Restore from Snapshot, pause ingestion (buffer via SQS). Cost Overrun: Temporarily lock new uploads, limit daily query quotas. 8. Expected Outcomes Technical Improvements\nTransform scattered document repositories (PDF/Scan) into digital knowledge queryable and automatically citable. Significantly reduce manual search time thanks to RAG + IDP technology. Long-term Value\nBuild a digitized research platform for 50+ researchers, easily scalable. Create a foundation for advanced features: Document recommendations, research trend analysis, and Literature Review support. Attachment You can download the detailed ARC Team proposal here:\nDownload Proposal of ARC Team.docx\n"},{"uri":"https://crystaljohn.github.io/fcj-workshop/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Week 10 Objectives: Setup AWS Account, IAM Users \u0026amp; Policies Research Bedrock Claude 3.5 \u0026amp; Cohere Embed APIs Research Textract AnalyzeDocument API Design architecture Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Setup AWS Account - Create IAM Users \u0026amp; Policies - Configure access permissions for AI/ML services 11/10/2025 11/10/2025 https://docs.aws.amazon.com/IAM/ 3 - Research Amazon Bedrock - Learn about Claude 3.5 model - Learn about Cohere Embed APIs 11/11/2025 11/11/2025 https://docs.aws.amazon.com/bedrock/ 4 - Write sample code test_bedrock.py - Test Claude 3.5 API calls - Test Cohere Embedding 11/12/2025 11/12/2025 https://docs.aws.amazon.com/bedrock/ 5 - Research Amazon Textract - Learn about AnalyzeDocument API - Write sample code test_textract.py 11/13/2025 11/13/2025 https://docs.aws.amazon.com/textract/ 6 - Design Architecture diagram - Estimate costs (~$65/month) - Review and finalize documentation 11/14/2025 11/14/2025 Week 10 Achievements: AWS Account Setup:\nCreated AWS Account 427995028618 with IAM users Configured IAM Policies for Bedrock, Textract Sample Code:\ntest_bedrock.py — test Claude 3.5 \u0026amp; Cohere Embed APIs test_textract.py — test AnalyzeDocument API Architecture:\nCompleted Architecture diagram for ARC-Chatbot Identified required services: Bedrock, Textract, S3, Lambda, \u0026hellip; Cost Estimation:\nEstimated costs approximately ~$65/month for dev/test environment "},{"uri":"https://crystaljohn.github.io/fcj-workshop/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":"Week 11 Objectives: Complete base infrastructure setup (M0) — S3, DynamoDB, Cognito, ALB Start implementing IDP Pipeline (M1) — SQS, PDF Detection, PyPDF2 extraction Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Setup S3 bucket arc-chatbot-documents-427995028618 - Create DynamoDB tables (metadata, chat-history) - Configure table schema and indexes 11/17/2025 11/17/2025 https://docs.aws.amazon.com/dynamodb/ 3 - Configure Cognito User Pool ap-southeast-1_8KB4JYvsX - Setup user authentication flow - Test sign-up/sign-in process 11/18/2025 11/18/2025 https://docs.aws.amazon.com/cognito/ 4 - Setup ALB arc-chatbot-dev-alb with health checks - Configure target groups and listeners - Test load balancer routing 11/19/2025 11/19/2025 https://docs.aws.amazon.com/elasticloadbalancing/ 5 - Create SQS queue arc-chatbot-dev-document-processing - Implement PDF detection service (digital vs scanned) - Write unit tests for PDF detector 11/20/2025 11/20/2025 https://docs.aws.amazon.com/sqs/ 6 - Implement PyPDF2 extraction for digital PDFs - Handle edge cases (encrypted, corrupted PDFs) - Write tests and achieve 30 tests passed 11/21/2025 11/21/2025 https://pypdf2.readthedocs.io/ Week 11 Achievements: Completed Base Infrastructure (M0):\nS3 bucket arc-chatbot-documents-427995028618 — document storage DynamoDB tables: metadata (document info), chat-history (conversation logs) Cognito User Pool ap-southeast-1_8KB4JYvsX — user authentication ALB arc-chatbot-dev-alb with health checks — load balancing for API Started IDP Pipeline (M1):\nSQS Queue arc-chatbot-dev-document-processing — document processing queue PDF Detector service — distinguish digital vs scanned PDFs (17 tests passed) PDF Extractor service with PyPDF2 — extract text from digital PDFs (30 tests passed) Learned:\nHow to design DynamoDB schema for chatbot application Cognito authentication flow and JWT tokens SQS message processing patterns Handling different types of PDFs in Python "},{"uri":"https://crystaljohn.github.io/fcj-workshop/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":"Week 12 Objectives: Complete RAG Chat component (M2) — Rate limiting, fallback, error handling Start Testing \u0026amp; Golive (M3) — Login page, Chat UI, Admin dashboard Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Implement Rate Limiter (60 RPM, 100K TPM) - Add Budget Manager ($10/day, $200/month) - Write tests for rate limiting logic 11/24/2025 11/24/2025 https://docs.aws.amazon.com/bedrock/ 3 - Implement fallback to Claude Haiku - Add error handling \u0026amp; retry logic with exponential backoff - Write tests (35 tests passed) 11/25/2025 11/25/2025 https://docs.aws.amazon.com/bedrock/ 4 - Implement login page with Cognito - Build AuthService with JWT validation - Test authentication flow end-to-end 11/26/2025 11/26/2025 https://docs.aws.amazon.com/cognito/ 5 - Build Chat interface UI - Implement ChatPage with message bubbles, citations - Create CitationCard, DocumentViewerModal components 11/27/2025 11/27/2025 https://react.dev/ 6 - Build Admin dashboard with drag-drop upload - Show document processing status (real-time auto-refresh) - Start E2E integration testing 11/28/2025 11/28/2025 https://react.dev/ Week 12 Achievements: Completed RAG Chat (M2):\nRate Limiter Budget Manager: $10/day, $200/month limit (22 tests passed) Bedrock Retry with exponential backoff (35 tests passed) Fallback from Claude 3.5 Sonnet → Claude Haiku when rate limited Started Testing \u0026amp; Golive (M3):\nAuthService with JWT validation — user authentication via Cognito ChatPage with message bubbles and citations — main chat interface CitationCard, DocumentViewerModal components — display document sources AdminPage with drag-drop upload — document management Real-time status monitoring with auto-refresh Learned:\nRate limiting patterns for AI APIs (token-based \u0026amp; request-based) Cost management strategies for Bedrock (budget alerts, fallback models) Exponential backoff with jitter for retry logic React components for chat UI (message streaming, citations) Cognito JWT validation in frontend "},{"uri":"https://crystaljohn.github.io/fcj-workshop/3-blogstranslated/3.3-blog3/","title":"Building Golden Images with CIS Linux Build Kit in Amazon EC2 Image Builder","tags":[],"description":"","content":"By: Rajat Chatterjee Date: May 11, 2025\nTopics: Amazon Inspector, Enterprise Governance and Control, Expert (400), Management Tools, Technical How-to\nIntroduction Building and deploying hardened and security-certified operating systems (OS) is an essential requirement for any Cloud Operations (CloudOps) or Cloud Center of Excellence (CCoE) team in an organization. The security guidelines and controls used to certify these images typically come from internal security teams, based on widely recognized industry standards.\nHowever, you also need the ability to control the hardening process, with flexibility to choose appropriate remediation steps and application deployment scenarios. The result is the formation of \u0026ldquo;Golden Amazon Machine Images (AMI)\u0026rdquo; – images that Business Units use directly.\nThis process requires automation, so that building and testing can occur at scale and images are distributed through approved Amazon Machine Images (AMI). One of the popular standards used as a baseline for image hardening is the Center for Internet Security (CIS) — an organization that creates and maintains a set of configuration guidelines called CIS Benchmarks, providing best practice configurations for specific technologies such as operating systems, cloud platforms, applications, and databases.\nCIS Benchmarks are widely recognized in the industry and referenced by many standards such as PCI DSS, HIPAA, DoD Cloud Computing SRG, FISMA, DFARS, and FEDRAMP.\nIn this post, we choose the CIS benchmark for Amazon Linux, and build a process to create certified AMIs through automation that can operate at scale.\nYou can use one of two approaches to create Golden Images by applying CIS Benchmark for the corresponding operating system (OS) to harden the AMI:\nUse managed components with CIS AMI from AWS Marketplace – You will also have access to the accompanying hardening component, which runs scripts to apply CIS Benchmark Level 1 guidelines to your configuration. These components are owned and maintained by CIS to ensure they are always updated with the latest guidelines. (Details on this approach can be found in the post Building CIS hardened Golden Images and Pipelines with EC2 Image Builder).\nUse a self-managed hardening process with Amazon EC2 Image Builder – This approach provides flexibility for CloudOps teams to customize the hardening process, allowing them to exclude benchmark recommendations that may affect applications intended for deployment on instances using the AMI.\nYou can choose one of these two options after carefully analyzing the factors explained in the post How to Decide Between Building or Buying a CIS Hardened Image.\nIn this post, we will demonstrate how to use scripts published by CIS Linux Build Kit (LBK) to create a self-managed hardening and validation process using EC2 Image Builder and Amazon Inspector. The scripts needed to harden a specific operating system (according to CIS Benchmark) are published on the CIS Workbench page, downloadable if your organization is a CIS SecureSuite® member. These scripts can be reused with minimal modifications to create EC2 Image Builder Components, then used in Image Builder Pipelines.\nWe will implement this approach by creating a hardened Amazon Linux 2 AMI, and validating it using Amazon Inspector.\nSolution overview The solution consists of the following main steps:\nPrerequisites:\nHave an AWS account with appropriate permissions. Download, explore, and update the CIS LBK Utility. Create an Amazon S3 bucket as a staging environment to upload the CIS LBK Utility. Configure the S3 bucket to send notifications via Amazon EventBridge. Main deployment steps:\nBuild the Image Building Pipeline using EC2 Image Builder. Validate the image and generate reports using Amazon Inspector. Solution prerequisites Download, explore, and update CIS LBK Utility The LBK for Amazon Linux 2 can be downloaded from the CIS website. This toolkit is developed and maintained by the CIS community, currently distributed as the file amazon_linux_2.tar.gz.\nAfter extracting the tar file, you will see the following directory structure:\nFigure 1: CIS Linux Build Kit folder structure\nThe main directory contains the file amazon_linux_2.sh — the main script that calls sub-scripts in the \u0026ldquo;functions\u0026rdquo; directory. If you want to exclude certain recommendations, add them to the exclusion_list.txt file.\nUsage instructions and toolkit contents are described in detail in the Quick Start Guide included in the package. The amazon_linux_2.sh script includes two interactive steps when running. For automation, you should modify this script to hardcode the input values.\nSpecifically, you need to:\nAccept the Terms and Conditions – you should comment out this line in the script, and the CloudOps team can provide separate terms content for users in internal documentation.\nSelect the hardening profile – there are four profiles: L1S, L2S, L1W, L2W corresponding to Level 1/2 of CIS Benchmark, and Server (S) or Workstation (W) type.\nIn this example, we will use the L1S profile to harden Amazon Linux 2 according to CIS Benchmark Level 1.\nLines to modify in the script:\n#terms_of_use #Display CIS Linux Build Kit warning banner #WARBNR ---\u0026gt; Comment this line run_profile=L1S # Uncomment this line to specify the profile to run After editing, repackage the LBK file:\ntar cvf amazon_linux_2.tar.gz CIS-LBK Create staging S3 bucket and upload LBK Create an S3 bucket (e.g., cis-lbk-al2) using AWS Management Console or AWS CLI. This bucket is used to store the LBK script, serving EC2 Image Builder for downloading and uploading logs after execution.\naws s3api put-object --bucket cis-lbk-al2 --key amazon_linux_2.tar.gz --body amazon_linux_2.tar.gz Configure S3 bucket to send notifications via Amazon EventBridge Enable Amazon S3 Event Notifications for the bucket, selecting to send notifications to Amazon EventBridge. You can enable this feature in the Properties tab of the bucket.\nFigure 2: Enable sending notifications to EventBridge from S3 bucket\nDeployment steps Figure 3: Solution architecture\nBuilding Image Building Pipeline with EC2 Image Builder This solution uses the following AWS services:\nEC2 Image Builder: runs LBK from Amazon S3 to harden and package the AMI. EC2 Image Builder uses AWS Task Orchestrator and Executor (AWSTOE) to orchestrate complex workflows. You will create a custom component to call LBK from S3, while defining an image recipe as the base image and an image workflow that defines the sequence of build steps.\nAmazon Inspector: performs security checks (CIS scan) to validate the AMI after hardening.\nAmazon EventBridge: integrates natively with EC2 Image Builder and S3 to emit events. These events can be forwarded to Amazon SNS or AWS Lambda.\nAmazon SNS: used to send email notifications about approval steps.\nCreate custom Component for EC2 Image Builder You can use an AWS CloudFormation template (provided in the original post) to initialize a stack that creates all resources mentioned in the sections below.\nExample command to deploy the stack:\naws cloudformation create-stack --stack-name cis-al2-harden-stack \\ --template-body file://cis-lbk-automation-template.yml \\ --parameters This CloudFormation template will automatically create the components described in the solution architecture.\nIn this post, we define a custom component named CIS-AL2-L1S-Hardening, with the following characteristics:\nType: build Platform: Linux and compatible operating systems OS Version: Amazon Linux 2 Each component is defined by a YAML document, where the S3 bucket created in the prerequisite step is used to download scripts and upload logs. The following table lists a series of steps in the build phase.\nFigure 4: Build phase steps\nExample component definition:\nname: \u0026#39;CIS-Build-AL2-Level1\u0026#39; description: \u0026#39;Applies the L1S CIS settings for Amazon Linux 2 (AL2)\u0026#39; schemaVersion: 1.0 parameters: - StagingS3BucketName: type: string - FileName: type: string default: \u0026#39;amazon_linux_2.tar.gz\u0026#39; - Version: type: string default: \u0026#39;2024.2.0\u0026#39; - Level: type: string default: \u0026#39;L1S\u0026#39; phases: - name: build steps: - name: OperatingSystemRelease action: ExecuteBash inputs: commands: - | FILE=/etc/os-release if [ -e $FILE ]; then . $FILE echo \u0026#34;$ID${VERSION_ID:+.${VERSION_ID}}\u0026#34; else echo \u0026#34;The file $FILE does not exist. Exiting.\u0026#34; exit 1 fi - name: GetPackageManager action: ExecuteBash inputs: commands: - | RELEASE=\u0026#39;{{build.OperatingSystemRelease.outputs.stdout}}\u0026#39; case \u0026#34;${RELEASE}\u0026#34; in amzn*|centos*|rhel*) echo \u0026#39;yum\u0026#39; ;; ubuntu*) echo \u0026#39;apt\u0026#39; ;; *) echo \u0026#34;Operating System \u0026#39;${RELEASE}\u0026#39; is not supported. Exiting.\u0026#34; exit 1 ;; esac - name: VerifyPrerequisite action: ExecuteBash onFailure: Abort inputs: commands: - | INSTALL_TYPE=\u0026#39;{{build.GetPackageManager.outputs.stdout}}\u0026#39; case \u0026#34;${INSTALL_TYPE}\u0026#34; in \u0026#39;yum\u0026#39;) if ! yum -q list installed tar \u0026amp;\u0026gt;/dev/null; then yum install -q -y tar echo \u0026#34;Installed tar.\u0026#34; else echo \u0026#34;Tar is already installed.\u0026#34; fi ;; \u0026#39;apt\u0026#39;) apt install -q -y tar echo \u0026#34;Installed tar.\u0026#34; ;; *) echo \u0026#34;Install type \u0026#39;${INSTALL_TYPE}\u0026#39; is not supported at this time. Exiting.\u0026#34; exit 1 ;; esac - name: MakeStagingDIR action: ExecuteBash inputs: commands: - | mkdir cis-stage cd cis-stage pwd - name: SettingStagingDirPermissions action: SetFolderPermissions inputs: - path: \u0026#39;{{build.MakeStagingDIR.outputs.stdout}}\u0026#39; permissions: 0700 - name: Download_CIS_LBK action: S3Download inputs: - source: s3://{{ StagingS3BucketName }}/{{ FileName }} destination: \u0026#39;{{build.MakeStagingDIR.outputs.stdout}}/{{ FileName }}\u0026#39; - name: Unzip_CIS_LBK action: ExecuteBash onFailure: Continue inputs: commands: - sudo tar -xvf \u0026#39;{{build.MakeStagingDIR.outputs.stdout}}/{{ FileName }}\u0026#39; -C \u0026#39;{{build.MakeStagingDIR.outputs.stdout}}/\u0026#39; || ( echo \u0026#34;File extraction failed. Exiting\u0026#34; ; exit 1; ) - name: AL2_ConfigureCIS action: ExecuteBash onFailure: Continue inputs: commands: - | RELEASE=\u0026#39;{{ build.OperatingSystemRelease.outputs.stdout }}\u0026#39; if [ ! `echo \u0026#34;$RELEASE\u0026#34; | grep -E \u0026#34;^(amzn\\.2$|(centos|rhel)\\.7)\u0026#34;` ]; then echo \u0026#39;Skipping this step for the current operating system.\u0026#39; exit 0 fi cd {{build.MakeStagingDIR.outputs.stdout}}/CIS-LBK/cis_lbk_amazon_linux_2 sudo ./amazon_linux_2.sh || ( echo \u0026#34;CIS configuration script failed to run. Exiting.\u0026#34; ; exit 1; ) - name: UploadLogFiles action: S3Upload onFailure: Abort maxAttempts: 3 inputs: - source: \u0026#39;{{build.MakeStagingDIR.outputs.stdout}}/CIS-LBK/cis_lbk_amazon_linux_2/logs/*\u0026#39; destination: \u0026#39;s3://{{ StagingS3BucketName }}/logs/\u0026#39; recurse: true expectedBucketOwner: \u0026lt;\u0026lt;your account number\u0026gt;\u0026gt; Create Image Recipe based on Component An Image Recipe is a document that defines the components applied to a base image to create the desired configuration for the output image.\nThe recipe named CIS-AL2-L1S-Hardening-Recipe is created using the custom component CIS-AL2-L1S-Hardening as the main build phase.\nSample AWS CloudFormation template is as follows:\nImageRecipeCISAL2L1S: Type: \u0026#34;AWS::ImageBuilder::ImageRecipe\u0026#34; Properties: Components: - ComponentArn: !GetAtt CISAL2HardeningComponent.Arn Parameters: - Value: - !Sub \u0026#34;${StagingS3BucketName}\u0026#34; Name: \u0026#34;StagingS3BucketName\u0026#34; WorkingDirectory: \u0026#34;/var/tmp\u0026#34; ParentImage: !Sub \u0026#34;arn:aws:imagebuilder:${AWS::Region}:aws:image/amazon-linux-2-x86/x.x.x\u0026#34; Version: \u0026#34;1.0.0\u0026#34; AdditionalInstanceConfiguration: SystemsManagerAgent: UninstallAfterBuild: true Name: \u0026#34;CIS-AL2-L1S-Hardening\u0026#34; DependsOn: - CISAL2HardeningComponent As you can see, the custom component (custom component) is referenced in the recipe, while the recipe also references the latest version of Amazon Linux 2 as the base image through the ParentImage property.\nThe CIS-AL2-L1S-Hardening component takes WorkingDirectory as input.\nThis value must be set to /var/tmp, because the default /tmp directory will have its permissions changed by the CIS LBK script, causing access errors when running.\nCreate custom Workflow for Image Build process Image Workflow defines the sequence of steps EC2 Image Builder performs during the build and test phases of the image creation process.\nThis workflow introduces the WaitForAction action, which adds a manual approval step after the hardening process completes. At this step, operators can review logs and continue creating the image. This is also when you can trigger and view Amazon Inspector scan results.\nThe WaitForAction action pauses the running workflow and waits for a response from the SendWorkflowStepAction API of EC2 Image Builder. This event emits an EventBridge event with detail-type \u0026ldquo;EC2 Image Builder Workflow Step Waiting\u0026rdquo;.\nFor this blog post, the authors create a custom workflow named build-cis-hardened-manual-action, type BUILD, with the following YAML content:\nname: build-cis-hardened-manual-action description: Workflow to build an AMI schemaVersion: 1.0 steps: - name: LaunchBuildInstance action: LaunchInstance onFailure: Abort inputs: waitFor: \u0026#34;ssmAgent\u0026#34; - name: ApplyBuildComponents action: ExecuteComponents onFailure: Abort inputs: instanceId.$: \u0026#34;$.stepOutputs.LaunchBuildInstance.instanceId\u0026#34; - name: InventoryCollection action: CollectImageMetadata onFailure: Abort if: and: - stringEquals: \u0026#34;AMI\u0026#34; value: \u0026#34;$.imagebuilder.imageType\u0026#34; - booleanEquals: true value: \u0026#34;$.imagebuilder.collectImageMetadata\u0026#34; inputs: instanceId.$: \u0026#34;$.stepOutputs.LaunchBuildInstance.instanceId\u0026#34; - name: RunSanitizeScript action: SanitizeInstance onFailure: Abort if: and: - stringEquals: \u0026#34;AMI\u0026#34; value: \u0026#34;$.imagebuilder.imageType\u0026#34; - stringEquals: \u0026#34;Linux\u0026#34; value: \u0026#34;$.imagebuilder.platform\u0026#34; inputs: instanceId.$: \u0026#34;$.stepOutputs.LaunchBuildInstance.instanceId\u0026#34; - name: CheckBuildLogsAndApprove action: WaitForAction - name: CreateOutputAMI action: CreateImage onFailure: Abort if: stringEquals: \u0026#34;AMI\u0026#34; value: \u0026#34;$.imagebuilder.imageType\u0026#34; inputs: instanceId.$: \u0026#34;$.stepOutputs.LaunchBuildInstance.instanceId\u0026#34; - name: TerminateBuildInstance action: TerminateInstance onFailure: Continue inputs: instanceId.$: \u0026#34;$.stepOutputs.LaunchBuildInstance.instanceId\u0026#34; outputs: - name: \u0026#34;ImageId\u0026#34; value: \u0026#34;$.stepOutputs.CreateOutputAMI.imageId\u0026#34; Create Image Pipeline In the blog example, the authors create an Image Pipeline named CIS-AL2-L1S-Hardening-pipeline.\nEC2 Image Builder uses Amazon Inspector to scan test instances for security vulnerabilities in the operating system or software packages. However, in this case, the default vulnerability scanning feature is disabled, as the team only uses Amazon Inspector to perform CIS scan on the output image, not general vulnerability scanning.\nThe Build Schedule is set to Manual mode. You can schedule it periodically using Schedule Builder (CRON expression).\nNext, the team selects the Image Recipe (CIS-AL2-L1S-Hardening-Recipe) created in the previous step. The image creation process is defined using a Custom Workflow, where build-cis-hardened-manual-action is selected as the Build workflow.\nThe IAM Role selected is AWSServiceRoleForImageBuilder, which provides the necessary access for EC2 Image Builder.\nThe Infrastructure Configuration is kept at default, except for adding a Resource Tag for the instance:\nKey: IBCISHardened Value: Yes → This tag is used in the Amazon Inspector configuration as a filter criterion for CIS scanning.\nDistribution Settings are kept at default – meaning EC2 Image Builder will launch instances in the default VPC, and distribute the output AMI in the current Region.\nThis Infrastructure Configuration section creates the IAM Role and Instance Profile used by the build instance to configure the AMI. This Instance Role is attached to EC2InstanceProfileForImageBuilder and AmazonSSMManagedInstanceCore managed policy.\nTo successfully run Amazon Inspector CIS scan, the Instance Profile needs to attach the additional policy AmazonInspector2ManagedCisPolicy.\nThe IAM Instance Profile Role configuration is as follows:\nIAMManagedPolicyIBCisComponentS3Access: Type: \u0026#34;AWS::IAM::ManagedPolicy\u0026#34; Properties: ManagedPolicyName: \u0026#34;IBCisComponentS3Access\u0026#34; Path: \u0026#34;/\u0026#34; Description: \u0026#34;Access to S3 Bucket from EC2 ImageBuilder CIS Custom component\u0026#34; PolicyDocument: Version: \u0026#34;2012-10-17\u0026#34; Statement: - Resource: \u0026#34;arn:aws:s3:::cis-lbk-al2/*\u0026#34; Action: - \u0026#34;s3:PutObject\u0026#34; - \u0026#34;s3:GetObject\u0026#34; Effect: \u0026#34;Allow\u0026#34; IAMRoleForImageBuilderCISAL2: Type: \u0026#34;AWS::IAM::Role\u0026#34; Properties: ManagedPolicyArns: - \u0026#34;arn:aws:iam::aws:policy/EC2InstanceProfileForImageBuilder\u0026#34; - Ref: \u0026#34;IAMManagedPolicyIBCisComponentS3Access\u0026#34; - \u0026#34;arn:aws:iam::aws:policy/AmazonInspector2ManagedCisPolicy\u0026#34; - \u0026#34;arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore\u0026#34; AssumeRolePolicyDocument: Version: \u0026#34;2012-10-17\u0026#34; Statement: - Action: \u0026#34;sts:AssumeRole\u0026#34; Effect: \u0026#34;Allow\u0026#34; Principal: Service: \u0026#34;ec2.amazonaws.com\u0026#34; EC2InstanceProfileForImageBuilderCISAL2: Type: \u0026#34;AWS::IAM::InstanceProfile\u0026#34; Properties: Roles: - !Ref \u0026#34;IAMRoleForImageBuilderCISAL2\u0026#34; InstanceProfileName: \u0026#34;EC2InstanceProfileForImageBuilderCISAL2\u0026#34; Attach Bucket Policy to S3 Bucket cis-lbk-al2 This policy only allows the IAM Role of the Instance Profile to access the bucket:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;Statement1\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;AWS\u0026#34;: \u0026#34;arn:aws:iam::7284282*****:role/IAMRoleForImageBuilderCISAL2\u0026#34; }, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:PutObject\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::cis-lbk-al2\u0026#34; } ] } Create Amazon SNS Topic for Email Notification The team creates an SNS Topic named ImageBuilderQueue, along with an Email Subscription to send notifications to the person responsible for the process.\nCreate Amazon EventBridge Rules to send notifications Two EventBridge rules are created to send notifications via SNS:\nRule 1 – Notification when workflow is waiting for approval (WaitForAction):\nWhen EC2 Image Builder emits a \u0026ldquo;Workflow Step Waiting\u0026rdquo; event for the CheckBuildLogsAndApprove step, this rule sends a message to the SNS Topic ImageBuilderQueue.\nAWSTemplateFormatVersion: \u0026#39;2010-09-09\u0026#39; Description: CloudFormation template for EventBridge Rule ec2-imagebuilder Resources: Rule41742a2e: Type: AWS::Events::Rule Properties: Name: ec2-imagebuilder EventPattern: \u0026gt;- {\u0026#34;source\u0026#34;:[\u0026#34;aws.imagebuilder\u0026#34;],\u0026#34;detail-type\u0026#34;:[\u0026#34;EC2 Image Builder Workflow Step Waiting\u0026#34;], \u0026#34;detail\u0026#34;:{\u0026#34;workflow-step-name\u0026#34;:[\u0026#34;CheckBuildLogsAndApprove\u0026#34;]}} State: ENABLED EventBusName: default Targets: - Arn: !Sub \u0026#34;arn:${AWS::Partition}:sns:${AWS::Region}:${AWS::AccountId}:ImageBuilderQueue\u0026#34; Rule 2 – Notification when CIS log is created in S3 bucket:\nWhen a CIS-LBK.log object is created in the /logs/.../ path, EventBridge sends a notification to the SNS topic ImageBuilderQueue.\nAWSTemplateFormatVersion: \u0026#39;2010-09-09\u0026#39; Description: CloudFormation template for EventBridge Rule CISHardeningExitLogEvent Resources: Rule104d463b: Type: AWS::Events::Rule Properties: Name: CISHardeningExitLogEvent EventPattern: \u0026gt;- {\u0026#34;source\u0026#34;:[\u0026#34;aws.s3\u0026#34;],\u0026#34;detail-type\u0026#34;:[\u0026#34;Object Created\u0026#34;], \u0026#34;detail\u0026#34;:{\u0026#34;bucket\u0026#34;:{\u0026#34;name\u0026#34;:[\u0026#34;cis-lbk-al2\u0026#34;]}, \u0026#34;object\u0026#34;:{\u0026#34;key\u0026#34;:[{\u0026#34;wildcard\u0026#34;:\u0026#34;logs/*CIS-LBK.log\u0026#34;}]}}} State: ENABLED EventBusName: default Targets: - Arn: !Sub \u0026#34;arn:${AWS::Partition}:sns:${AWS::Region}:${AWS::AccountId}:ImageBuilderQueue\u0026#34; InputTransformer: InputPathsMap: bucketname: $.detail.bucket.name objectkey: $.detail.object.key region: $.region InputTemplate: \u0026gt;- \u0026#34;Review the output log for the CIS Hardening process in \u0026lt;bucketname\u0026gt; bucket at \u0026lt;objectkey\u0026gt;. Log URL: https://\u0026lt;bucketname\u0026gt;.s3.\u0026lt;region\u0026gt;.amazonaws.com/\u0026lt;objectkey\u0026gt;\u0026#34; Run Image Pipeline You can launch the pipeline from AWS Console or AWS CLI.\nSample CLI command:\naws imagebuilder start-image-pipeline-execution \\ --image-pipeline-arn arn:aws:imagebuilder:{REGION}:{ACCOUNT_ID}:image-pipeline/CIS-AL2-L1S-Hardening-pipeline Response:\n{ \u0026#34;requestId\u0026#34;: \u0026#34;a1b2c3d4-5678-90ab-cdef-EXAMPLE11111\u0026#34;, \u0026#34;clientToken\u0026#34;: \u0026#34;a1b2c3d4-5678-90ab-cdef-EXAMPLE22222\u0026#34;, \u0026#34;imageBuildVersionArn\u0026#34;: \u0026#34;arn:aws:imagebuilder:{REGION}:{ACCOUNT_ID}:image/cis-al2-l1s-hardening/1.0.1/1\u0026#34; } Note the imageBuildVersionArn and stepExecutionId. The pipeline will pause at the CheckBuildLogsAndApprove step.\nCheck the waiting status with the command:\naws imagebuilder list-waiting-workflow-step When running successfully, you will receive 2 email notifications:\nOne email from S3 EventBridge containing the CIS-LBK log file link. One email from Image Builder requesting approval for the waiting step. Example log notification content:\n\u0026#34;Review the output log for the CIS Hardening process in \u0026#39;cis-lbk-al2\u0026#39; bucket, location \u0026#39;logs/08_27_2024_1210/CIS-LBK.log\u0026#39;...\u0026#34; Testing Image with Amazon Inspector During the waiting phase, you create a one-time CIS scan using Amazon Inspector:\naws inspector2 create-cis-scan-configuration \\ --scan-name cis-hardened-scan \\ --schedule oneTime={} \\ --security-level \u0026#34;LEVEL_1\u0026#34; \\ --targets \u0026#34;accountIds=\u0026lt;\u0026lt;account_id\u0026gt;\u0026gt;,targetResourceTags={IBCISHardened=Yes}\u0026#34; Monitor scan progress:\naws inspector2 list-cis-scans \\ --filter-criteria \u0026#34;scanConfigurationArnFilters=[{comparison=EQUALS,value=\u0026lt;\u0026lt;scanConfigurationArn\u0026gt;\u0026gt;}]\u0026#34; After completion, download the PDF report:\naws inspector2 get-cis-scan-report --scan-arn \u0026lt;\u0026lt;scan-arn\u0026gt;\u0026gt; --report-format PDF The result returns the URL of the report, which can be opened in a browser logged into AWS.\nContinue or stop workflow After reviewing logs and reports:\nContinue pipeline:\naws imagebuilder send-workflow-step-action \\ --step-execution-id \u0026lt;\u0026lt;stepExecutionId\u0026gt;\u0026gt; \\ --image-build-version-arn \u0026lt;\u0026lt;imageBuildVersionArn\u0026gt;\u0026gt; \\ --action RESUME Or stop pipeline:\naws imagebuilder send-workflow-step-action \\ --step-execution-id \u0026lt;\u0026lt;stepExecutionId\u0026gt;\u0026gt; \\ --image-build-version-arn \u0026lt;\u0026lt;imageBuildVersionArn\u0026gt;\u0026gt; \\ --action STOP \\ --reason \u0026#34;Please fix the issue and recreate the Image\u0026#34; When complete, view the output AMI:\naws imagebuilder get-image \\ --image-build-version-arn \u0026lt;\u0026lt;imageBuildVersionArn\u0026gt;\u0026gt; \\ --query \u0026#39;image.outputResources.amis[0].image\u0026#39; Conclusion In this post, we set up a process based on EC2 Image Builder to harden an Amazon Linux 2 AMI according to the CIS Amazon Linux 2 Benchmark using the CIS Linux Build Kit provided by the CIS community.\nWe also leveraged event-based notifications from Amazon S3 and Amazon EventBridge to trigger approval emails and introduce a manual approval process.\nThis framework can be extended to harden other AMIs such as Ubuntu or CentOS, using the corresponding LBK kits from the CIS community.\nContact your AWS representative for support in implementing this process for your organization.\nFurther reading CIS hardening components for EC2 Image Builder Build a pipeline for hardened container images using EC2 Image Builder and Terraform Amazon Inspector Best Practices About the author Rajat Chatterjee\nRajat is a Senior Solutions Architect, working with customers in the Financial Services sector including Banking, Insurance, and Capital Markets, helping them build reliable, secure, and high-performance applications on the cloud.\nHe has expertise in large-scale deployments of microservices-based workloads on container platforms for enterprises.\n"},{"uri":"https://crystaljohn.github.io/fcj-workshop/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"Blog 1 - Building a Virtual Meteorologist Using Amazon Bedrock Agents This article presents how to build a virtual meteorologist using Amazon Bedrock Agents, helping to answer weather-related questions in natural language. You will be guided on how to configure the agent, define \u0026ldquo;action groups\u0026rdquo; to handle geographical location, time, and weather data; while integrating AWS services such as Lambda, Cognito, and Amplify for end-to-end deployment.\nBlog 2 - Build End-to-End Apache Spark Pipelines with Amazon MWAA, Batch Processing Gateway, and Amazon EMR on EKS This article guides how to build end-to-end Spark pipelines by combining Amazon MWAA (Managed Workflows for Apache Airflow) with Batch Processing Gateway to orchestrate Spark jobs across multiple EMR clusters running on EKS. You will learn about integration, multi-cluster processing architecture, job routing and monitoring methods, as well as how to deploy a real-world environment for large-scale data systems.\nBlog 3 - Building Golden Images with CIS Linux Build Kit in Amazon EC2 Image Builder This article presents how to use CIS Linux Build Kit (LBK) to automate the hardening process and validate Amazon Machine Images (AMIs) through EC2 Image Builder. You will be guided through two approaches — using available CIS components from AWS Marketplace or self-managing LBK scripts — to create \u0026ldquo;Golden Images\u0026rdquo; according to CIS Level 1 standards, while integrating Amazon Inspector and EventBridge for monitoring, enabling notifications \u0026amp; reviewing results.\n"},{"uri":"https://crystaljohn.github.io/fcj-workshop/5-workshop/3-bedrock-models/","title":"Activate Bedrock Models","tags":[],"description":"","content":"Before deploying the solution, you need to activate the necessary Amazon Bedrock models in your AWS account.\nSteps to Activate Models Search for Amazon Bedrock in AWS Console Access Model catalog from the left navigation menu Select the corresponding model names: Anthropic Claude 3.5 Sonnet Anthropic Claude 3 Sonnet Anthropic Claude 3 Haiku Cohere - Embed Multilingual v3 Select \u0026ldquo;Open in playground\u0026rdquo; and send a test message to activate each model Note: Make sure you activate all four models in the ap-southeast-1 (Singapore) region as the solution is deployed in this region.\n"},{"uri":"https://crystaljohn.github.io/fcj-workshop/5-workshop/4-aws-cli/","title":"Configure AWS CLI","tags":[],"description":"","content":"To deploy and manage the solution, you need to configure the AWS Command Line Interface (AWS CLI) with your authentication information.\nSteps Step 1: Check if AWS CLI is installed aws --version If not installed:\n# Download and install MSI installer msiexec.exe /i https://awscli.amazonaws.com/AWSCLIV2.msi Step 2: Create IAM User (if not already exists) Login to AWS Console Search \u0026ldquo;IAM\u0026rdquo; → Click IAM Left sidebar → Users → Create user User name: arc-workshop-user Click Next Attach policies directly, select policies: AmazonEC2FullAccess AmazonS3FullAccess AmazonDynamoDBFullAccess AmazonCognitoPowerUser AmazonSQSFullAccess AmazonTextractFullAccess AmazonBedrockFullAccess CloudWatchFullAccess IAMFullAccess Click Create user Step 3: Create Access Key Go to the created user → Tab Security credentials Scroll down Access keys → Click Create access key Select Command Line Interface (CLI) Tick \u0026ldquo;I understand\u0026hellip;\u0026rdquo; → Next Description: ARC Workshop CLI Click Create access key ⚠️ IMPORTANT: Copy or download .csv file\nAccess key ID: AKIAXXXXXXXXXXXXXXXX Secret access key: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx Step 4: Configure AWS CLI Open PowerShell and run:\naws configure Enter information:\nAWS Access Key ID [None]: AKIAXXXXXXXXXXXXXXXX AWS Secret Access Key [None]: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx Default region name [None]: ap-southeast-1 Default output format [None]: json Step 5: Verify Configuration Check identity:\naws sts get-caller-identity Expected output:\n{ \u0026#34;UserId\u0026#34;: \u0026#34;AIDAXXXXXXXXXXXXXXXXX\u0026#34;, \u0026#34;Account\u0026#34;: \u0026#34;123456789012\u0026#34;, \u0026#34;Arn\u0026#34;: \u0026#34;arn:aws:iam::123456789012:user/arc-workshop-user\u0026#34; } "},{"uri":"https://crystaljohn.github.io/fcj-workshop/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy it verbatim for your report, including this warning.\nIn this section, you should list and describe in detail the events you have participated in during your internship or work experience.\nEach event should be presented in the format Event 1, Event 2, Event 3…, along with the following details:\nEvent name Date and time Location (if applicable) Your role in the event (attendee, event support, speaker, etc.) A brief description of the event’s content and main activities Outcomes or value gained (lessons learned, new skills, contribution to the team/project) This listing helps demonstrate your actual participation as well as the soft skills and experience you have gained from each event. During my internship, I participated in 4 events with opportunities provided by AWS. Each event brought me new perspectives and motivation to seek and conquer new knowledge and challenges. Along with that came networking and interaction with AWS community members.\nEvent 1 Event Name: BUILDING AGENTIC AI\nDate \u0026amp; Time: 05/12/2025\nLocation: Bitexco Financial Tower, 26th Floor, 2 Hai Trieu, District 1, Ho Chi Minh City\nRole: Attendee\nMain Content: Context optimization with Amazon Bedrock, building AI agent automation, RAG techniques and prompt engineering\nEvent 2 Event Name: AWS Cloud Mastery Series #3 workshop (Security Pillar)\nDate \u0026amp; Time: 29/11/2025\nLocation: Bitexco Financial Tower, 26th Floor, 2 Hai Trieu, District 1, Ho Chi Minh City\nRole: Attendee\nMain Content: AWS Well-Architected Framework – Security Pillar, IAM, Detection \u0026amp; Monitoring, Infrastructure Protection, Data Protection, Incident Response\nEvent 3 Event Name: AWS Cloud Mastery Series #2 workshop (DevOps)\nDate \u0026amp; Time: 17/11/2025\nLocation: Bitexco Financial Tower, 26th Floor, 2 Hai Trieu, District 1, Ho Chi Minh City\nRole: Attendee\nMain Content: Modern DevOps mindset, CI/CD Pipeline, Infrastructure as Code (IaC), Container Services (ECS/EKS/App Runner), Monitoring \u0026amp; Observability\nEvent 4 Event Name: AWS Cloud Mastery Series #1 workshop (AI/ML/GenAI)\nDate \u0026amp; Time: 15/11/2025\nLocation: Bitexco Financial Tower, 26th Floor, 2 Hai Trieu, District 1, Ho Chi Minh City\nRole: Attendee\nMain Content: Amazon SageMaker, Generative AI with Amazon Bedrock, Bedrock Agents, MLOps, CI/CD workflow for containers\n"},{"uri":"https://crystaljohn.github.io/fcj-workshop/5-workshop/5-data-preparation/","title":"Data Preparation","tags":[],"description":"","content":"Data Preparation Before creating AWS resources, you need to download sample data to test the system.\nStep 1: Download Sample Data Access ARC Sample Data Download the data to your computer Extract the file, which will create a folder named DATA Document Requirements Limit Value Format PDF (text-based or scanned) Max size 50 MB Max pages 500 pages Recommended 10-100 pages Prepare AWS Resources Step 2: Create S3 Bucket S3 Bucket is used to store uploaded PDF documents.\nSearch for S3 in AWS Console Click Create bucket Configure bucket: Bucket name: arc-documents-\u0026lt;YOUR-ACCOUNT-ID\u0026gt; (replace \u0026lt;YOUR-ACCOUNT-ID\u0026gt; with your AWS Account ID) AWS Region: Asia Pacific (Singapore) ap-southeast-1 Keep other settings as default Click Create bucket 💡 Tip: To get your AWS Account ID, run:\naws sts get-caller-identity --query Account --output text Or create using CLI:\naws s3 mb s3://arc-documents-$(aws sts get-caller-identity --query Account --output text) --region ap-southeast-1 Step 3: Create DynamoDB Table DynamoDB Table is used to store document metadata.\nSearch for DynamoDB in AWS Console Click Create table Configure table: Table name: arc-documents Partition key: doc_id (String) Sort key: sk (String) Table settings: Default settings Click Create table Or create using CLI:\naws dynamodb create-table \\ --table-name arc-documents \\ --attribute-definitions \\ AttributeName=doc_id,AttributeType=S \\ AttributeName=sk,AttributeType=S \\ --key-schema \\ AttributeName=doc_id,KeyType=HASH \\ AttributeName=sk,KeyType=RANGE \\ --billing-mode PAY_PER_REQUEST \\ --region ap-southeast-1 Step 4: Create SQS Queue SQS Queue is used for IDP pipeline to process documents.\nSearch for SQS in AWS Console Click Create queue Configure queue: Type: Standard Name: arc-document-queue Keep other settings as default Click Create queue Or create using CLI:\naws sqs create-queue --queue-name arc-document-queue --region ap-southeast-1 Step 5: Verify Resources Check that all resources have been created:\n# S3 Bucket aws s3 ls | grep arc-documents # DynamoDB Table aws dynamodb describe-table --table-name arc-documents --region ap-southeast-1 --query \u0026#34;Table.TableName\u0026#34; # SQS Queue aws sqs get-queue-url --queue-name arc-document-queue --region ap-southeast-1 Step 6: Upload Data to S3 Upload PDF files from the DATA folder downloaded in Step 1:\n# Upload all files from DATA folder aws s3 cp DATA/ s3://arc-documents-\u0026lt;YOUR-ACCOUNT-ID\u0026gt;/uploads/ --recursive # Or upload individual file aws s3 cp DATA/sample-document.pdf s3://arc-documents-\u0026lt;YOUR-ACCOUNT-ID\u0026gt;/uploads/ 💡 Tip: Replace \u0026lt;YOUR-ACCOUNT-ID\u0026gt; with your AWS Account ID\nVerify upload:\naws s3 ls s3://arc-documents-\u0026lt;YOUR-ACCOUNT-ID\u0026gt;/uploads/ Checklist Before proceeding, make sure:\nAWS CLI installed and configured Terraform installed Docker installed and running Node.js 18+ installed Python 3.11+ installed Git installed Repository cloned IAM user created with sufficient permissions Bedrock models approved (Claude + Cohere) S3 Bucket created DynamoDB Table created SQS Queue created Sample documents uploaded to S3 "},{"uri":"https://crystaljohn.github.io/fcj-workshop/5-workshop/","title":"Workshop","tags":[],"description":"","content":"AI-POWERED ACADEMIC RESEARCH CHATBOT ON AWS Overview In this workshop, we will build ARC (Academic Research Chatbot) - an intelligent chatbot system running on the AWS Serverless platform. This solution leverages Generative AI and RAG (Retrieval-Augmented Generation) to support academic research, document queries, and answer questions flexibly.\nInstead of answering questions based on fixed scripts (rule-based), the system uses the Claude 3.5 Sonnet model to understand natural language, query data from the vector database, and respond to users accurately.\nWorkshop Objectives After completing this workshop, you will:\nUnderstand RAG architecture and how to apply it in practice Deploy a complete chatbot system on AWS Use Amazon Bedrock (Claude 3.5 Sonnet + Cohere Embed) Build an IDP pipeline with Amazon Textract Implement vector search with Qdrant Deploy infrastructure with Terraform Integrate authentication with Amazon Cognito Content Introduction Preparation Activate Bedrock Models Configure AWS CLI Data Preparation Deploy Infrastructure Setup Backend API Setup IDP Pipeline Setup Frontend Using Chatbot Using Admin Dashboard Clean up Resources "},{"uri":"https://crystaljohn.github.io/fcj-workshop/5-workshop/6-infrastructure/","title":"Deploy Infrastructure","tags":[],"description":"","content":"In this section, we will clone the repository and deploy the entire AWS infrastructure for the ARC Chatbot system.\nStep 1: Clone Repository Clone the repository from GitHub:\ngit clone https://github.com/CrystalJohn/ARC-project.git cd ARC-project Step 2: Build Dashboard Before deploying the application, we need to build the frontend dashboard.\nNavigate to the frontend folder cd frontend Install dependencies Run the following command to install the necessary libraries:\nnpm install Build Dashboard After installation is complete, run the build command:\nnpm run build After the process is complete, a dist folder will be created. Verify the index.html file and the assets folder:\nls dist/ # index.html assets/ Return to the project root directory cd .. Step 3: Deploy Terraform Application Deploy the Terraform application. The process will take approximately 20-30 minutes to deploy all resources.\ncd terraform terraform init terraform apply --auto-approve ⚠️ Note: If you encounter an error at this step, make sure Docker is running on your computer.\n💡 Info: Replace \u0026lt;account_id\u0026gt; with your actual AWS Account ID.\nStep 4: Verify Deployment After completing all the steps above, your environment has been successfully deployed.\nYou can verify the deployment by checking:\nAWS Console: Check the resources that have been created (EC2, S3, Cognito, DynamoDB, etc.) Terraform State: Run terraform state list to see the list of resources S3 Buckets: Buckets for documents and frontend have been created EC2 Instance: Instance for backend has been launched Check Outputs terraform output Important outputs:\nOutput Description api_endpoint Backend API URL cognito_user_pool_id Cognito User Pool ID cognito_client_id Cognito App Client ID s3_bucket_name S3 bucket for documents cloudfront_url Frontend URL Next Steps Now you can proceed to:\nSet up Backend Set up IDP Pipeline Set up Frontend "},{"uri":"https://crystaljohn.github.io/fcj-workshop/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"During my internship at AWS - Amazon Web Services from 12/08/2025 to 12/11/2025, I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in the ARC-Chatbot project - Academic Research Chatbot using Generative AI and RAG on AWS Serverless platform, through which I improved my skills in Python programming, system architecture design, using AWS services (Bedrock, Textract, S3, DynamoDB, Cognito), technical documentation writing, and teamwork.\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ✅ ☐ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ✅ ☐ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ✅ ☐ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ✅ ☐ ☐ 5 Discipline Adhering to schedules, rules, and work processes ☐ ✅ ☐ 6 Progressive mindset Willingness to receive feedback and improve oneself ✅ ☐ ☐ 7 Communication Presenting ideas and reporting work clearly ☐ ✅ ☐ 8 Teamwork Working effectively with colleagues and participating in teams ✅ ☐ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ☐ ✅ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ✅ ☐ ☐ 12 Overall General evaluation of the entire internship period ✅ ☐ ☐ Needs Improvement Strengthen discipline and strictly comply with the rules and regulations of the company or any organization Improve problem-solving thinking Enhance system architecture design skills to deliver more effective solutions that provide greater value to users "},{"uri":"https://crystaljohn.github.io/fcj-workshop/5-workshop/7-backend/","title":"Set up Backend API","tags":[],"description":"","content":"Set up Backend API In this section, you will configure the Backend API (FastAPI) and Qdrant vector database on EC2.\nBackend Architecture Internet → ALB (:80) → EC2 Private Subnet ├── FastAPI Container (:8000) ├── Qdrant Container (:6333) └── SQS Worker (background) 💡 Note: EC2 is located in a Private Subnet with no Public IP. Access via SSM Session Manager.\nStep 1: Access EC2 via Session Manager The EC2 instance was created in a Private Subnet with no Public IP. Use AWS Systems Manager Session Manager to access it.\nMethod 1: AWS Console Open AWS Console → EC2 → Instances Select instance arc-dev-app-server Click Connect → Session Manager → Connect Method 2: AWS CLI # Get Instance ID from Terraform output INSTANCE_ID=$(terraform -chdir=terraform output -raw ec2_instance_id) # Connect via SSM aws ssm start-session --target $INSTANCE_ID --region ap-southeast-1 ⚠️ Requirement: Install Session Manager Plugin\nStep 2: Check Running Services EC2 has been automatically set up via user_data script when Terraform created the instance. Verify the services:\n# Switch to ec2-user sudo su - ec2-user # Check Docker containers docker ps You should see 2 containers running:\napp-fastapi-1 - FastAPI server (port 8000) app-qdrant-1 - Qdrant vector database (port 6333) # Check Qdrant curl http://localhost:6333/collections # Check FastAPI curl http://localhost:8000/health Step 3: Deploy Backend Code Backend code will be deployed via CI/CD Pipeline (CodePipeline → CodeBuild → CodeDeploy). However, for quick testing, you can deploy manually:\ncd /home/ec2-user # Clone repository git clone https://github.com/CrystalJohn/ARC-project.git cd ARC-project/backend # Stop old containers cd /home/ec2-user/app docker-compose down # Copy backend code cp -r /home/ec2-user/ARC-project/backend/* /home/ec2-user/app/ # Start with new code docker-compose up -d --build Step 4: Configure Environment Variables Create a .env file with values from Terraform outputs:\ncd /home/ec2-user/app # Get values from Terraform outputs (run on local machine) # terraform -chdir=terraform output cat \u0026gt; .env \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; # AWS Configuration AWS_REGION=ap-southeast-1 # S3 S3_BUCKET_NAME=arc-documents-\u0026lt;account-id\u0026gt; # DynamoDB DYNAMODB_TABLE_NAME=arc-dev-documents # SQS SQS_QUEUE_URL=https://sqs.ap-southeast-1.amazonaws.com/\u0026lt;account-id\u0026gt;/arc-dev-document-queue # Qdrant (local container) QDRANT_HOST=qdrant QDRANT_PORT=6333 # Cognito COGNITO_USER_POOL_ID=ap-southeast-1_xxxxx COGNITO_CLIENT_ID=xxxxx # Bedrock BEDROCK_MODEL_ID=anthropic.claude-3-5-sonnet-20241022-v2:0 EMBEDDING_MODEL_ID=amazon.titan-embed-text-v2:0 EOF 💡 Tip: Replace \u0026lt;account-id\u0026gt; and xxxxx values with actual outputs from Terraform.\nStep 5: Restart Services # Restart to load new .env docker-compose down docker-compose up -d # Check logs docker-compose logs -f fastapi Step 6: Verify via ALB Backend is exposed via Application Load Balancer. Verify from your local machine:\n# Get ALB DNS from Terraform output ALB_DNS=$(terraform -chdir=terraform output -raw alb_dns_name) # Test health endpoint curl http://$ALB_DNS/health # {\u0026#34;status\u0026#34;:\u0026#34;healthy\u0026#34;} Step 7: Check Qdrant Collection # On EC2 curl http://localhost:6333/collections # Create collection for documents (if not exists) curl -X PUT http://localhost:6333/collections/documents \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;vectors\u0026#34;: { \u0026#34;size\u0026#34;: 1024, \u0026#34;distance\u0026#34;: \u0026#34;Cosine\u0026#34; } }\u0026#39; 💡 Note: Vector size 1024 corresponds to Amazon Titan Embeddings v2.\nChecklist Successfully accessed EC2 via Session Manager Docker containers running (fastapi, qdrant) .env file configured Health check via ALB successful Qdrant collection created Troubleshooting Unable to connect to Session Manager # Check SSM Agent on EC2 sudo systemctl status amazon-ssm-agent # Verify IAM Role has AmazonSSMManagedInstanceCore policy Container fails to start # View logs docker-compose logs # Check disk space df -h ALB health check fails # Verify Security Group allows port 8000 from ALB # Verify FastAPI is listening on 0.0.0.0:8000 docker-compose logs fastapi "},{"uri":"https://crystaljohn.github.io/fcj-workshop/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":"Overall Evaluation 1. Working Environment The First Cloud Journey program is truly an amazing experience. The program was designed to help me understand and experience AWS services (one of the most powerful cloud platforms). The comprehensive 12-week curriculum allowed me to progress systematically from basic AWS knowledge to deploying the ARC-Chatbot project — an MVP RAG Chatbot application. The weekly worklog structure helped me track work progress and learning status effectively. The environment encourages self-learning while providing adequate resources and support when needed.\n2. Mentor / Team Support The guidance and team collaboration throughout the program was excellent. The team was always ready to discuss technical challenges, review my architecture proposals, and guide me through complex AWS concepts like Bedrock, Textract, and RAG + IDP pipeline. What I appreciated most was that the mentor always encouraged me to research and propose solutions before providing assistance — helping me develop independent thinking.\n3. Relevance of Work to Academic Major The program content aligns well with modern cloud computing and software engineering practices. Starting from networking fundamentals (VPC, Security Groups) and progressing to serverless architecture, databases, security, and AI/ML services helped me build a comprehensive cloud engineering skillset.\nThe hands-on project ARC-Chatbot — building an AI-integrated chatbot with admin dashboard, IDP pipeline, and RAG chat — gave me practical experience directly applicable to real-world scenarios. The emphasis on Infrastructure as Code (Terraform, CDK) and DevOps practices perfectly aligns with current industry standards.\n4. Learning \u0026amp; Skill Development Opportunities The 12-week journey provided me with numerous learning opportunities across many aspects:\nTechnical Skills: Learned to use over 15 AWS services (Bedrock, Textract, SageMaker, DynamoDB, S3, Cognito, SQS, Lambda, ECS, ECR, ALB, CloudWatch, X-Ray, VPC, IAM\u0026hellip;) AI/ML Integration: Learned to integrate Claude 3.5, Cohere Embeddings, RAG pipeline, and Qdrant vector database Architecture Design: Learned to design scalable, cost-effective cloud solutions Documentation: Improved technical writing skills through weekly worklogs and translating AWS blogs Problem Solving: Developed debugging skills by solving issues like rate limiting, PDF processing, and vector search optimization Cost Optimization: Learned to make architecture decisions based on cost-benefit analysis (~$65/month estimation) The progression from AWS Fundamentals (Week 1-4) → S3 \u0026amp; Security (Week 5) → Serverless (Week 6) → Containers (Week 7) → AI/ML (Week 8) → Architecture Design (Week 9) → Full Implementation (Week 10-12) was well-paced and logical.\n5. Culture \u0026amp; Team Spirit The FCJ program nurtures a culture of continuous learning, collaboration, and innovation. Team meetings were constructive, with open discussions about technical trade-offs and architecture decisions. Workshops like \u0026ldquo;AWS Cloud Mastery Series\u0026rdquo; and \u0026ldquo;Building Agentic AI\u0026rdquo; provided opportunities to expand knowledge beyond the core curriculum.\n6. Program Policies / Support The program structure was well-designed with clear weekly objectives, comprehensive reference materials (AWS Study Group, Cloud Journey, official AWS docs), and hands-on labs. The flexibility to explore different approaches (multiple database options, vector search, PDF extraction) while staying focused on the main project was valuable.\nAccess to AWS accounts and guidance on budget management helped learn cost optimization from day one.\nDetailed Feedback on Program Phases Phase 1 (Week 1-4): AWS Fundamentals \u0026amp; Core Services\nStrengths: Comprehensive and systematic introduction to AWS, VPC, EC2, Linux deployment. Hands-on labs reinforced theoretical knowledge. Phase 2 (Week 5-6): Security, Serverless \u0026amp; Databases\nStrengths: Learned S3, IAM, Security best practices (Week 5). Progressed to Lambda, API Gateway, DynamoDB (Week 6). Phase 3 (Week 7-8): Containers \u0026amp; AI/ML Services\nStrengths: Learned Docker, ECR, ECS (Week 7). Studied Bedrock, Textract, and AI/ML services (Week 8). Phase 4 (Week 9): Architecture Design\nStrengths: Designed architecture for ARC-Chatbot project, deployment planning, and cost estimation. Phase 5 (Week 10-12): Full Implementation \u0026amp; Testing\nStrengths: Real deployment — Assessment M0 (Week 10), IDP Pipeline M1 (Week 11), RAG Chat M2 \u0026amp; Testing M3 (Week 12). End-to-end deployment experience was extremely valuable. What I\u0026rsquo;m Most Satisfied With Production-ready Solution: Successfully deployed a complete ARC-Chatbot system from scratch, including IDP pipeline, RAG chat, admin dashboard with 200+ tests passed.\nTechnical Achievements:\nPDF Detector service (17 tests passed) PDF Extractor with PyPDF2 (30 tests passed) Qdrant vector search (35 tests passed) Claude Service with streaming (20 tests passed) Rate Limiter \u0026amp; Budget Manager (45 tests passed) Bedrock Retry with exponential backoff (35 tests passed) Problem-Solving Victories: Overcame technical challenges like digital vs scanned PDF detection, text chunking optimization, and rate limiting for AI APIs.\nMeasurable Progress: From AWS beginner in Week 1 to confidently deploying multi-service AI/ML architectures by Week 12.\nWould I Recommend This Program? Absolutely! I highly recommend the First Cloud Journey program to anyone interested in cloud computing for many reasons:\nComprehensive Curriculum: Covers all essential AWS service categories from basic to advanced Hands-on Approach: Real project deployment provides practical experience beyond certifications Structured Progression: 12-week timeframe with clear milestones maintains motivation Supportive Community: Mentors and team members actively help with technical challenges Career Alignment: Skills learned (IaC, serverless, security, AI/ML, cost optimization) directly apply to industry positions Flexibility: Program allows exploring different services while focusing on core objectives \u0026ldquo;The program helped me transition from theoretical understanding to practical deployment capability.\u0026rdquo;\nSuggestions \u0026amp; Expectations for Future Cohorts Suggestions to improve the internship experience:\nBetter Pre-Internship Preparation\nProvide reading lists or video series on basic cloud concepts Set up AWS accounts and CLI before Week 1 to maximize hands-on time Weekly Retrospectives\nStructured reflection sessions each week to discuss challenges and lessons Share common issues and solutions across all interns Guest Speaker Sessions\nInvite AWS Solutions Architects or cloud experts to share real-world experiences Industry insights on emerging services and best practices Capstone Presentation\nFormal presentation of final project to mentors and peers Practice communicating technical decisions to various audiences Alumni Network\nConnect with previous FCJ program participants Mentorship opportunities for future interns Would I like to continue this program in the future?\nYes, I would love to continue with advanced programs such as:\nAdvanced AWS Specialization: Machine Learning, Data Analytics, or Security specialty tracks AWS Certification Preparation: Solutions Architect Professional, DevOps Engineer Open Source Contribution: Contributing to AWS CDK constructs or community projects Other Comments The First Cloud Journey program exceeded my expectations in every way. Learning from AWS fundamentals to deploying a complete ARC-Chatbot solution in 12 weeks truly boosted my confidence. The emphasis on hands-on learning, cost optimization, and best practices prepared me well for a career in cloud computing.\nWhat I treasured most about the program:\nThe freedom to explore different architecture approaches and learn from mistakes The supportive team environment that encouraged questions and creative solutions The requirement for thorough documentation significantly improved my technical writing skills The real project requiring integration of multiple AWS services (Bedrock, Textract, DynamoDB, S3, Cognito, SQS, ECS\u0026hellip;) Thank you to the FCJ team, mentors, and fellow interns for this wonderful learning experience. The knowledge and skills gained during these 12 weeks will be a solid foundation for my future career in cloud computing and software engineering.\nEvaluation Summary Category Rating Working Environment ⭐⭐⭐⭐⭐ (5/5) Mentor Support ⭐⭐⭐⭐⭐ (5/5) Program Relevance ⭐⭐⭐⭐⭐ (5/5) Learning Opportunities ⭐⭐⭐⭐⭐ (5/5) Team Culture ⭐⭐⭐⭐⭐ (5/5) Program Structure ⭐⭐⭐⭐☆ (4.5/5) Overall Rating ⭐⭐⭐⭐⭐ (5/5) "},{"uri":"https://crystaljohn.github.io/fcj-workshop/5-workshop/8-idp-pipeline/","title":"Set up IDP Pipeline","tags":[],"description":"","content":"Set up IDP Pipeline In this section, you will setup SQS Worker to process documents through IDP (Intelligent Document Processing) pipeline.\nIDP Flow Upload → S3 → DynamoDB (UPLOADED) → SQS ↓ EC2 Worker ↓ PyPDF2 (digital) / Textract (scanned) ↓ Chunk Text (1000 tokens) ↓ Cohere Embed Multilingual v3 (Bedrock) ↓ Qdrant (store vectors) ↓ DynamoDB (EMBEDDING_DONE) 💡 Note: Worker uses PyPDF2 for digital PDF (text-based) and Textract for scanned PDF (image-based).\nDocument States Status Description UPLOADED File uploaded, waiting for processing IDP_RUNNING Worker is processing TEXTRACT_DONE OCR completed (scanned PDF only) EMBEDDING_DONE Completed, ready to use FAILED Error occurred Step 1: Access EC2 via Session Manager # Get Instance ID INSTANCE_ID=$(terraform -chdir=terraform output -raw ec2_instance_id) # Connect aws ssm start-session --target $INSTANCE_ID --region ap-southeast-1 After connecting:\nsudo su - ec2-user cd /home/ec2-user/backend 💡 Note: EC2 has 2 folders:\napp/ - Boilerplate from user_data script backend/ - Actual code deployed via CI/CD (contains run_worker.py) Step 2: Check Worker Code Worker code is in backend/run_worker.py. Verify the file exists:\nls -la # Must have: run_worker.py, app/, requirements.txt Step 3: Configure Environment Ensure .env file has all variables (in backend/ folder):\ncd /home/ec2-user/backend cat .env Important variables for IDP:\nSQS_QUEUE_URL=https://sqs.ap-southeast-1.amazonaws.com/\u0026lt;account\u0026gt;/arc-dev-document-queue S3_BUCKET=arc-documents-\u0026lt;account\u0026gt; QDRANT_HOST=localhost QDRANT_PORT=6333 AWS_REGION=ap-southeast-1 Step 4: Start Worker Option A: Run directly (for debugging) # Activate virtual environment (if available) source venv/bin/activate # Run worker python run_worker.py Worker will display:\n============================================================ IDP Pipeline - SQS Worker ============================================================ Queue URL: https://sqs.ap-southeast-1.amazonaws.com/xxx/arc-dev-document-queue Bucket: arc-documents-xxx Region: ap-southeast-1 Qdrant: localhost:6333 ------------------------------------------------------------ Processing indefinitely (Ctrl+C to stop)... Option B: Run in background with nohup nohup python run_worker.py \u0026gt; worker.log 2\u0026gt;\u0026amp;1 \u0026amp; # Check process ps aux | grep run_worker # View logs tail -f worker.log Option C: Run in Docker (recommended) # Add worker to docker-compose.yml docker-compose up -d worker Step 5: Test IDP Pipeline 5.1 Upload test file to S3 # From local machine aws s3 cp test-sample.pdf s3://arc-documents-\u0026lt;account\u0026gt;/uploads/test-001.pdf 5.2 Create record in DynamoDB aws dynamodb put-item \\ --table-name arc-dev-documents \\ --item \u0026#39;{ \u0026#34;doc_id\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;test-001\u0026#34;}, \u0026#34;sk\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;METADATA\u0026#34;}, \u0026#34;filename\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;test-sample.pdf\u0026#34;}, \u0026#34;s3_key\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;uploads/test-001.pdf\u0026#34;}, \u0026#34;status\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;UPLOADED\u0026#34;}, \u0026#34;uploaded_at\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;\u0026#39;$(date -u +%Y-%m-%dT%H:%M:%SZ)\u0026#39;\u0026#34;} }\u0026#39; 5.3 Send message to SQS aws sqs send-message \\ --queue-url https://sqs.ap-southeast-1.amazonaws.com/\u0026lt;account\u0026gt;/arc-dev-document-queue \\ --message-body \u0026#39;{ \u0026#34;doc_id\u0026#34;: \u0026#34;test-001\u0026#34;, \u0026#34;s3_key\u0026#34;: \u0026#34;uploads/test-001.pdf\u0026#34;, \u0026#34;filename\u0026#34;: \u0026#34;test-sample.pdf\u0026#34; }\u0026#39; Step 6: Monitor Processing View worker logs:\n# If running directly # Logs displayed on terminal # If running in background tail -f worker.log Successful logs will look like:\n2024-01-15 10:30:00 - INFO - Received message for doc_id: test-001 2024-01-15 10:30:01 - INFO - Downloading from S3: uploads/test-001.pdf 2024-01-15 10:30:02 - INFO - Extracting text with PyPDF2... 2024-01-15 10:30:03 - INFO - Created 8 chunks from document 2024-01-15 10:30:05 - INFO - Generating embeddings with Cohere... 2024-01-15 10:30:10 - INFO - Stored 8 vectors for test-001 2024-01-15 10:30:10 - INFO - Updated status: EMBEDDING_DONE 2024-01-15 10:30:10 - INFO - Document test-001 processed successfully Step 7: Verify Processing 7.1 Check DynamoDB aws dynamodb get-item \\ --table-name arc-dev-documents \\ --key \u0026#39;{\u0026#34;doc_id\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;test-001\u0026#34;},\u0026#34;sk\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;METADATA\u0026#34;}}\u0026#39; \\ --query \u0026#39;Item.{status:status.S,chunks:chunk_count.N}\u0026#39; Expected output:\n{ \u0026#34;status\u0026#34;: \u0026#34;EMBEDDING_DONE\u0026#34;, \u0026#34;chunks\u0026#34;: \u0026#34;8\u0026#34; } 7.2 Check Qdrant # On EC2 curl -s http://localhost:6333/collections/arc_documents/points/count | jq Expected output:\n{ \u0026#34;result\u0026#34;: { \u0026#34;count\u0026#34;: 8 } } Error Handling Issue Cause Solution Worker not receiving messages SQS URL incorrect Check .env Bedrock timeout Rate limit Increase retry delay Qdrant connection refused Container not started docker-compose up -d qdrant FAILED status Check error_message in DynamoDB Fix and retry Retry Failed Document # Update status back to UPLOADED to retry aws dynamodb update-item \\ --table-name arc-dev-documents \\ --key \u0026#39;{\u0026#34;doc_id\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;test-001\u0026#34;},\u0026#34;sk\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;METADATA\u0026#34;}}\u0026#39; \\ --update-expression \u0026#34;SET #s = :s\u0026#34; \\ --expression-attribute-names \u0026#39;{\u0026#34;#s\u0026#34;:\u0026#34;status\u0026#34;}\u0026#39; \\ --expression-attribute-values \u0026#39;{\u0026#34;:s\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;UPLOADED\u0026#34;}}\u0026#39; # Send message to SQS again aws sqs send-message \\ --queue-url $SQS_QUEUE_URL \\ --message-body \u0026#39;{\u0026#34;doc_id\u0026#34;:\u0026#34;test-001\u0026#34;,\u0026#34;s3_key\u0026#34;:\u0026#34;uploads/test-001.pdf\u0026#34;,\u0026#34;filename\u0026#34;:\u0026#34;test-sample.pdf\u0026#34;}\u0026#39; Checklist Access EC2 via Session Manager Worker code is available Environment variables configured Worker is running Test document uploaded to S3 SQS message sent Worker processed document (logs) Status = EMBEDDING_DONE in DynamoDB Vectors stored in Qdrant "},{"uri":"https://crystaljohn.github.io/fcj-workshop/5-workshop/9-frontend/","title":"Setup Frontend","tags":[],"description":"","content":"Setup Frontend Configure and deploy Frontend React application with AWS Amplify.\nStep 1: Get Terraform Outputs cd terraform terraform output Note: cognito_user_pool_id, cognito_client_id, alb_dns_name\nStep 2: Configure Environment cd ARC-project cp .env.example .env Edit .env:\nVITE_AWS_REGION=ap-southeast-1 VITE_COGNITO_POOL_ID=ap-southeast-1_xxxxxxx VITE_COGNITO_CLIENT_ID=xxxxxxxxxxxxxxxxxxxxxxxxxx VITE_API_URL=http://arc-chatbot-dev-alb-xxxxx.ap-southeast-1.elb.amazonaws.com Step 3: Install \u0026amp; Test Local npm install npm run dev Open http://localhost:5173\nStep 4: Build \u0026amp; Deploy npm run build Push code to GitHub, Amplify will deploy automatically:\ngit add . git commit -m \u0026#34;Update frontend config\u0026#34; git push origin main 💡 Amplify app was created via Terraform and connected with GitHub.\nStep 5: Update Cognito Callback URLs After getting Amplify URL:\naws cognito-idp update-user-pool-client \\ --user-pool-id ap-southeast-1_xxxxxxx \\ --client-id xxxxxxxxxx \\ --callback-urls \u0026#34;http://localhost:5173\u0026#34; \u0026#34;https://main.xxxxx.amplifyapp.com\u0026#34; \\ --logout-urls \u0026#34;http://localhost:5173\u0026#34; \u0026#34;https://main.xxxxx.amplifyapp.com\u0026#34; Step 6: Create Test Users # Admin user aws cognito-idp admin-create-user \\ --user-pool-id ap-southeast-1_xxxxxxx \\ --username admin@example.com \\ --user-attributes Name=email,Value=admin@example.com \\ --temporary-password \u0026#34;TempPass123!\u0026#34; aws cognito-idp admin-add-user-to-group \\ --user-pool-id ap-southeast-1_xxxxxxx \\ --username admin@example.com \\ --group-name admin Error Handling Error Solution CORS error Check FastAPI CORS config \u0026ldquo;User pool does not exist\u0026rdquo; Check VITE_COGNITO_POOL_ID Build failed Check Amplify environment variables Checklist .env configured Local dev server running Amplify deploy successful Cognito callback URLs updated Login/Register working "},{"uri":"https://crystaljohn.github.io/fcj-workshop/5-workshop/10-using-chatbot/","title":"Using Chatbot","tags":[],"description":"","content":"Guide to using ARC Chatbot to search for information from research documents.\nAccess Local: http://localhost:5173 Production: Amplify URL from previous step Step 1: Login Enter email and password Click Login Redirect to Chat page Step 2: Chat Interface After logging in, you will see:\nSidebar with Chat, History menus Header with user info and dark mode toggle Chat area with welcome message Step 3: Ask Questions Enter a question in the input box and press Enter or click Send.\nGood Questions Type Example Definition \u0026ldquo;What is a stack data structure?\u0026rdquo; Comparison \u0026ldquo;Compare stack and queue\u0026rdquo; Explanation \u0026ldquo;Explain binary search algorithm\u0026rdquo; Avoid ❌ Too general: \u0026ldquo;Tell me about programming\u0026rdquo; ❌ Outside documents: \u0026ldquo;What\u0026rsquo;s the weather today?\u0026rdquo; Step 4: Citations Each answer has citations showing document sources:\n📚 Sources: [1] data-structures.pdf - Page 12 - Score: 85% [2] algorithms.pdf - Page 45 - Score: 72% Click on citation to view document details.\nField Description [1], [2] Citation number Filename PDF filename Page Page number Score Relevance (%) Step 5: Conversation History Click History in sidebar View list of previous conversations Click conversation to reload it Click trash icon to delete Step 6: New Chat Click New Chat in sidebar to start a new conversation.\nFeatures Feature Description Streaming Response displays in parts Markdown Supports code blocks, lists, headers Dark Mode Toggle in header History Save and reload conversations Checklist Successfully logged in Sent query and received response Citations displayed correctly Click citation to view document History working New Chat working "},{"uri":"https://crystaljohn.github.io/fcj-workshop/5-workshop/11-admin-dashboard/","title":"Using Admin Dashboard","tags":[],"description":"","content":"Using Admin Dashboard Guide to using Admin Dashboard to manage documents.\nAccess URL: http://localhost:5173/admin (or Amplify URL) Requirement: Account in admin group Step 1: Login Admin Log in with admin account (created in previous step).\nStep 2: Dashboard Overview After logging in, you will see:\nUpload section (drag \u0026amp; drop) Documents table with pagination Status filter and auto-refresh Step 3: Upload Documents 3.1. Select files Drag \u0026amp; drop PDF files to upload area Or click Browse Files to select 3.2. Upload Progress Each file displays progress bar and status:\nuploading - Uploading success - Upload successful error - Upload failed Step 4: Document Status After upload, document will be processed through IDP pipeline:\nStatus Description Time UPLOADED Waiting for processing - IDP_RUNNING Processing 1-5 min EMBEDDING_DONE Ready - FAILED Error - 💡 Tip: Enable Auto-refresh (5s) to automatically update status.\nStep 5: Manage Documents Filter by Status Use Status dropdown to filter:\nAll Uploaded Processing Done Failed Pagination Documents are paginated (5 items/page). Use pagination controls at footer.\nView Document Click 👁️ icon to view document details.\nDelete Document Click 🗑️ icon to delete document.\n⚠️ Warning: Deleting document will delete from S3, DynamoDB, and Qdrant.\nStep 6: Processing History Click Processing History link to view document processing history.\nError Handling Issue Solution Upload failed Check file size (\u0026lt;50MB), format (PDF only) Document stuck in IDP_RUNNING Check worker logs on EC2 Document FAILED See error message in Processing History Checklist Logged in to admin dashboard Upload document successful Document processed (EMBEDDING_DONE) Filter/pagination working Auto-refresh working "},{"uri":"https://crystaljohn.github.io/fcj-workshop/5-workshop/12-cleanup/","title":"Cleanup Resources","tags":[],"description":"","content":"Cleanup Resources After completing the workshop, clean up AWS resources to avoid incurring charges.\n⚠️ Warning: These steps will PERMANENTLY DELETE all data and resources!\nCleanup Order Stop services on EC2 Empty S3 buckets Terraform destroy Verify cleanup Step 1: Stop Services on EC2 Connect EC2 via Session Manager:\nINSTANCE_ID=$(terraform -chdir=terraform output -raw ec2_instance_id) aws ssm start-session --target $INSTANCE_ID --region ap-southeast-1 Stop Docker containers:\nsudo su - ec2-user cd /home/ec2-user/app # Stop containers docker-compose down # Remove volumes docker volume rm app_qdrant_storage Step 2: Empty S3 Buckets S3 buckets must be empty before Terraform destroy:\n# Get bucket name from Terraform output BUCKET=$(terraform -chdir=terraform output -raw s3_bucket_name) # Empty bucket aws s3 rm s3://$BUCKET --recursive # Or force delete aws s3 rb s3://$BUCKET --force Step 3: Terraform Destroy cd terraform terraform plan -destroy terraform destroy Enter yes when prompted. This process takes about 10-15 minutes.\nStep 4: Manual Cleanup (if needed) If there are still resources not deleted:\n# CloudWatch Log Groups aws logs describe-log-groups --log-group-name-prefix /aws/arc | jq -r \u0026#39;.logGroups[].logGroupName\u0026#39; | xargs -I {} aws logs delete-log-group --log-group-name {} # EC2 Key Pair (if created manually) aws ec2 delete-key-pair --key-name arc-keypair # Amplify App (if created manually) aws amplify list-apps | jq -r \u0026#39;.apps[] | select(.name | contains(\u0026#34;arc\u0026#34;)) | .appId\u0026#39; | xargs -I {} aws amplify delete-app --app-id {} Step 5: Verify Cleanup # Check EC2 aws ec2 describe-instances --filters \u0026#34;Name=tag:Name,Values=*arc*\u0026#34; --query \u0026#39;Reservations[].Instances[].InstanceId\u0026#39; # Check S3 aws s3 ls | grep arc # Check RDS/DynamoDB aws dynamodb list-tables --query \u0026#39;TableNames[?contains(@, `arc`)]\u0026#39; # Check Lambda aws lambda list-functions --query \u0026#39;Functions[?contains(FunctionName, `arc`)].FunctionName\u0026#39; # Check ECR aws ecr describe-repositories --query \u0026#39;repositories[?contains(repositoryName, `arc`)].repositoryName\u0026#39; Expected output: All empty.\nCost Estimation Before cleanup, check estimated costs:\n# CloudWatch - Check billing dashboard for actual charges # https://console.aws.amazon.com/billing/ Checklist EC2 services stopped S3 buckets emptied Terraform destroy completed Manual cleanup verified All resources deleted Billing dashboard checked "},{"uri":"https://crystaljohn.github.io/fcj-workshop/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://crystaljohn.github.io/fcj-workshop/tags/","title":"Tags","tags":[],"description":"","content":""}]